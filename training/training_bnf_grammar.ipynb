{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded epoch_36\n"
     ]
    }
   ],
   "source": [
    "# Init/Load model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Define a directory to save the models\n",
    "SAVE_DIR = '../saved_models'\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "start_epoch = 36\n",
    "\n",
    "class SimpleBART(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleBART, self).__init__()\n",
    "        if start_epoch > 0:\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained(os.path.join(SAVE_DIR, f'epoch_{start_epoch}'))\n",
    "            print(f'Loaded epoch_{start_epoch}')\n",
    "        else:\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "            print('Loaded facebook/bart-base')\n",
    "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "model = SimpleBART().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Raw Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "\n",
    "ACCUMULATION_STEPS = 14\n",
    "BATCH_SIZE = 14 # best performing batch size so far (in execution performance)\n",
    "DATA_SIZE = 0\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=200):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.start_token_id = tokenizer.cls_token_id\n",
    "        self.end_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, tokens = self.data[idx]\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
    "        \n",
    "        # Add start and end tokens and then pad\n",
    "        tokens = [self.start_token_id] + tokens + [self.end_token_id]\n",
    "        tokens_padded = [self.pad_token_id] * self.max_length\n",
    "        tokens_padded[:len(tokens)] = tokens\n",
    "        tokens_padded[len(tokens):] = [self.pad_token_id] * (self.max_length - len(tokens))\n",
    "        \n",
    "        return inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), torch.tensor(tokens_padded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def load_data_from_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [(row[0], [int(tok) for tok in row[1].split(\",\")]) for row in reader]\n",
    "\n",
    "    return data\n",
    "\n",
    "def apply_concept(params, validation=False):\n",
    "    global validationLoader\n",
    "    global dataloader\n",
    "    global DATA_SIZE\n",
    "\n",
    "    merged_data = load_data_from_csv(f\"../concept/egg.csv\")\n",
    "    \n",
    "    print(\"Concepts loaded;\")\n",
    "    for file_name, percentage in params.items():\n",
    "        data = load_data_from_csv(f\"../concept/{file_name}.csv\")\n",
    "        cutoff = int(len(data) * percentage)\n",
    "        \n",
    "        if validation:\n",
    "            loaded_data = data[-cutoff:]\n",
    "        else:\n",
    "            loaded_data = data[:cutoff]\n",
    "\n",
    "        print(f\"    - {file_name}: {len(loaded_data)}\")\n",
    "        merged_data.extend(loaded_data)\n",
    "\n",
    "    DATA_SIZE = len(merged_data)\n",
    "    dataset = CustomDataset(merged_data, model.tokenizer)\n",
    "    if validation:\n",
    "        validationLoader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(f'  Total: {DATA_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Validator\n",
    "def validate():\n",
    "    EOS_TOKEN_ID = model.tokenizer.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for accuracy calculation\n",
    "    total_correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "\n",
    "    # Initialize counters for average sequence accuracy within the mask\n",
    "    total_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask, targets) in enumerate(validationLoader):\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Identify where the EOS token is in the target sequence\n",
    "            eos_positions = (targets == EOS_TOKEN_ID).cumsum(dim=1).type(torch.bool)\n",
    "            mask = ~eos_positions | (targets == EOS_TOKEN_ID)\n",
    "\n",
    "            _, predicted = logits.max(2)\n",
    "            correct_sequences = ((predicted == targets) | ~mask).all(dim=1).float().sum().item()\n",
    "            total_sequences += targets.size(0)\n",
    "            total_correct_sequences += correct_sequences\n",
    "\n",
    "            # Compute the accuracy for each sequence\n",
    "            correct_tokens_per_sequence = ((predicted == targets) & mask).float().sum(dim=1)\n",
    "            total_tokens_per_sequence = mask.float().sum(dim=1)\n",
    "            total_accuracy += (correct_tokens_per_sequence / total_tokens_per_sequence).sum().item()\n",
    "\n",
    "    # Compute and print the accuracy for the entire validation dataset\n",
    "    validation_accuracy = total_correct_sequences / total_sequences\n",
    "    print(f\"  Total Seq Acc: {validation_accuracy*100:.3f}%\")\n",
    "    avg_accuracy = total_accuracy / total_sequences\n",
    "    print(f\"    Avg Seq Acc: {avg_accuracy*100:.3f}%\")\n",
    "\n",
    "    return avg_accuracy, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "def save_model():\n",
    "    global start_epoch\n",
    "    model_save_path = os.path.join(SAVE_DIR, f'epoch_{start_epoch}')\n",
    "    model.bart.save_pretrained(model_save_path)\n",
    "\n",
    "def trainFor(num_epochs, target_loss=0, target_p50_loss=0, target_acc=1.0, target_seq_acc=1.0):\n",
    "    global start_epoch\n",
    "\n",
    "    EOS_TOKEN_ID = model.tokenizer.eos_token_id\n",
    "    acc_batch = int(ACCUMULATION_STEPS / BATCH_SIZE)\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for epoch in range(start_epoch+1, start_epoch+num_epochs+1):\n",
    "        start_epoch = epoch\n",
    "        model.train()\n",
    "\n",
    "        # Resetting the accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize counters for accuracy calculation\n",
    "        total_correct_sequences = 0\n",
    "        total_sequences = 0\n",
    "        cumulative_loss = 0.0\n",
    "\n",
    "        # Initialize list to store batch losses\n",
    "        batch_losses = []\n",
    "\n",
    "        for batch_idx, (input_ids, attention_mask, targets) in enumerate(dataloader):\n",
    "\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Identify where the EOS token is in the target sequence\n",
    "            eos_positions = (targets == EOS_TOKEN_ID).cumsum(dim=1).type(torch.bool)\n",
    "            mask = ~eos_positions | (targets == EOS_TOKEN_ID)\n",
    "\n",
    "            # Apply mask to filter out tokens after the EOS token for loss computation\n",
    "            active_loss = mask.view(-1).bool()\n",
    "            active_logits = logits.view(-1, logits.size(-1))[active_loss]\n",
    "            active_labels = targets.view(-1)[active_loss]\n",
    "            loss = criterion(active_logits, active_labels)\n",
    "\n",
    "            _, predicted = logits.max(2)\n",
    "            correct_sequences = ((predicted == targets) | ~mask).all(dim=1).float().sum().item()\n",
    "            total_sequences += targets.size(0)\n",
    "            total_correct_sequences += correct_sequences\n",
    "\n",
    "            # Accumulate the gradients\n",
    "            loss.backward()\n",
    "            loss_val = loss.item()\n",
    "\n",
    "            cumulative_loss += loss_val\n",
    "            batch_losses.append(loss_val)\n",
    "\n",
    "            isLast = batch_idx == len(dataloader) - 1\n",
    "\n",
    "            # Only perform an optimization step every ACCUMULATION_STEPS\n",
    "            if isLast or batch_idx % acc_batch == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            print(f\"\\rEpoch: {epoch}, Batch: {batch_idx} of {total_batches}, loss: {loss_val:.6f}      \", end='')\n",
    "\n",
    "\n",
    "        # Compute and print the accuracy for the entire epoch\n",
    "        epoch_accuracy = total_correct_sequences / total_sequences\n",
    "        cumulative_loss = cumulative_loss / len(batch_losses)\n",
    "        p25_loss = np.percentile(batch_losses, 25)\n",
    "        p50_loss = np.percentile(batch_losses, 50)\n",
    "        p75_loss = np.percentile(batch_losses, 75)\n",
    "        print(f\"\\rEpoch: {epoch}, Accuracy: {epoch_accuracy*100:.2f}%                                             \")\n",
    "        print(f\"  Loss: 25%: {p25_loss:.6f} 50%: {p50_loss:.6f} 75%: {p75_loss:.6f}\\n   Avg: {cumulative_loss:.6f}\")\n",
    "\n",
    "        if epoch % 10 == 0: # Save the model\n",
    "            save_model()\n",
    "\n",
    "        seq_acc, total_acc = validate()\n",
    "\n",
    "        if total_acc >= target_acc:\n",
    "            break\n",
    "        if cumulative_loss <= target_loss:\n",
    "            break\n",
    "        if p50_loss <= target_p50_loss:\n",
    "            break\n",
    "\n",
    "        if seq_acc >= target_seq_acc:\n",
    "            break\n",
    "\n",
    "    # Make sure last epoch is always saved\n",
    "    save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 3122\n",
      "  Total: 9931\n",
      "Epoch: 1, Accuracy: 0.01%\n",
      "  Loss: 25%: 4.393126 50%: 4.887876 75%: 5.315798\n",
      "   Avg: 4.884056\n",
      "  Total Seq Acc: 0.000%\n",
      "    Avg Seq Acc: 39.868%\n",
      "Epoch: 2, Accuracy: 0.03%\n",
      "  Loss: 25%: 4.117525 50%: 4.550774 75%: 4.877478\n",
      "   Avg: 4.482987\n",
      "  Total Seq Acc: 0.004%\n",
      "    Avg Seq Acc: 39.875%\n",
      "Epoch: 3, Accuracy: 0.03%\n",
      "  Loss: 25%: 4.068293 50%: 4.452379 75%: 4.818588\n",
      "   Avg: 4.398917\n",
      "  Total Seq Acc: 0.062%\n",
      "    Avg Seq Acc: 39.897%\n",
      "Epoch: 4, Accuracy: 0.05%\n",
      "  Loss: 25%: 4.008659 50%: 4.428133 75%: 4.759971\n",
      "   Avg: 4.360003\n",
      "  Total Seq Acc: 0.085%\n",
      "    Avg Seq Acc: 39.924%\n",
      "Epoch: 5, Accuracy: 0.16%\n",
      "  Loss: 25%: 3.946199 50%: 4.349040 75%: 4.712277\n",
      "   Avg: 4.299947\n",
      "  Total Seq Acc: 0.214%\n",
      "    Avg Seq Acc: 40.006%\n",
      "Epoch: 6, Accuracy: 0.48%\n",
      "  Loss: 25%: 3.817751 50%: 4.224310 75%: 4.577750\n",
      "   Avg: 4.175182\n",
      "  Total Seq Acc: 1.057%\n",
      "    Avg Seq Acc: 40.588%\n",
      "Epoch: 7, Accuracy: 2.34%\n",
      "  Loss: 25%: 3.542352 50%: 3.961223 75%: 4.297835\n",
      "   Avg: 3.902212\n",
      "  Total Seq Acc: 3.791%\n",
      "    Avg Seq Acc: 43.450%\n",
      "Epoch: 8, Accuracy: 6.20%\n",
      "  Loss: 25%: 3.052983 50%: 3.451971 75%: 3.804974\n",
      "   Avg: 3.416453\n",
      "  Total Seq Acc: 8.795%\n",
      "    Avg Seq Acc: 50.804%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = BATCH_SIZE # pure memorisation so accumulation won't help\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.02})\n",
    "trainFor(50, target_seq_acc=0.50)\n",
    "# (31mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 6245\n",
      "  Total: 13054\n",
      "Epoch: 9, Accuracy: 10.68%\n",
      "  Loss: 25%: 2.948524 50%: 3.301470 75%: 3.602107\n",
      "   Avg: 3.285144\n",
      "  Total Seq Acc: 16.104%\n",
      "    Avg Seq Acc: 63.344%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = BATCH_SIZE # pure memorisation so accumulation won't help\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.04})\n",
    "trainFor(50, target_seq_acc=0.60)\n",
    "# (4mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 9368\n",
      "  Total: 16177\n",
      "Epoch: 10, Accuracy: 14.79%\n",
      "  Loss: 25%: 2.290249 50%: 2.601158 75%: 2.916466\n",
      "   Avg: 2.600121\n",
      "  Total Seq Acc: 25.349%\n",
      "    Avg Seq Acc: 75.091%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 28 # *14 close to 32\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.06})\n",
    "trainFor(50, target_seq_acc=0.65)\n",
    "# (5mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aiming for:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{unique}{tokens} &= \\frac{4174}{6808} = 61.31\\%  & \\text{sign pairs to text only used once} \\\\\n",
    "  \\frac{text}{tokens}   &= \\frac{5342}{6808}  = 78.47\\% & \\text{sign pairs to text unique text} \\\\\n",
    "  & & \\text{unique meaning the text is only used for one tokenID}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "i.e. there are six different signs which can be used for \"present\"  \n",
    "which means we're actually aiming for $96\\%$ effective accuracy $\\frac{75\\%}{78\\%}$\n",
    "\n",
    "But we don't want to over-fit either  \n",
    "Hence why we slowly introduce new concepts while still memorising vocabulary\n",
    "\n",
    "`target_seq_acc` includes the `EOS` token, which we of course we want to be right\n",
    "So our target should be $\\frac{61.31\\% + 100\\%}{2} = 80.65\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 12491\n",
      "  Total: 19300\n",
      "Epoch: 11, Accuracy: 18.79%\n",
      "  Loss: 25%: 1.740285 50%: 1.988600 75%: 2.250141\n",
      "   Avg: 2.004923\n",
      "  Total Seq Acc: 31.401%\n",
      "    Avg Seq Acc: 79.550%\n",
      "Epoch: 12, Accuracy: 23.83%\n",
      "  Loss: 25%: 1.321281 50%: 1.537223 75%: 1.742366\n",
      "   Avg: 1.547170\n",
      "  Total Seq Acc: 35.058%\n",
      "    Avg Seq Acc: 81.933%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 70 # *14 close to 64\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.08})\n",
    "trainFor(50, target_seq_acc=0.80655)\n",
    "# (10mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "  Total Seq Acc: 35.058%\n",
      "    Avg Seq Acc: 81.933%\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 62.109%\n",
      "    Avg Seq Acc: 87.325%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 23.260%\n",
      "    Avg Seq Acc: 79.579%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.795788674411047, 0.23259686199167467)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure memorisation is over, as the vocabulary has been sufficiently learnt  \n",
    "While it's not perfect, it will improve further over the later training, as the vocab remains in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 35132\n",
      "  Total: 41941\n",
      "Epoch: 13, Accuracy: 19.62%\n",
      "  Loss: 25%: 1.276297 50%: 1.521877 75%: 1.806241\n",
      "   Avg: 1.567434\n",
      "  Total Seq Acc: 37.671%\n",
      "    Avg Seq Acc: 83.350%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.225})\n",
    "trainFor(50, target_seq_acc=0.6)\n",
    "# (10mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 64.576%\n",
      "    Avg Seq Acc: 88.098%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 25.937%\n",
      "    Avg Seq Acc: 81.276%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8127563179130933, 0.25936599423631124)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Break down of validation per concept\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 70264\n",
      "  Total: 77073\n",
      "Epoch: 14, Accuracy: 21.77%\n",
      "  Loss: 25%: 0.978782 50%: 1.179458 75%: 1.420422\n",
      "   Avg: 1.226024\n",
      "  Total Seq Acc: 42.332%\n",
      "    Avg Seq Acc: 86.016%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.45})\n",
    "trainFor(50, target_seq_acc=0.7)\n",
    "# (16mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "  Total Seq Acc: 42.332%\n",
      "    Avg Seq Acc: 86.016%\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 67.190%\n",
      "    Avg Seq Acc: 88.945%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 31.489%\n",
      "    Avg Seq Acc: 84.735%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8473481228669942, 0.3148895292987512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the model saved correctly\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 140528\n",
      "  Total: 147337\n",
      "Epoch: 15, Accuracy: 25.32%\n",
      "  Loss: 25%: 0.735360 50%: 0.893032 75%: 1.094783\n",
      "   Avg: 0.942284\n",
      "  Total Seq Acc: 46.069%\n",
      "    Avg Seq Acc: 87.619%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.9})\n",
    "trainFor(50, target_seq_acc=0.75)\n",
    "# (29mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "  Total Seq Acc: 46.069%\n",
      "    Avg Seq Acc: 87.619%\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 69.863%\n",
      "    Avg Seq Acc: 89.861%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 35.690%\n",
      "    Avg Seq Acc: 86.637%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8663697381680721, 0.3569004162664105)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 140528\n",
      "  Total: 147337\n",
      "Epoch: 16, Accuracy: 29.25%\n",
      "  Loss: 25%: 0.571872 50%: 0.698485 75%: 0.865685\n",
      "   Avg: 0.749544\n",
      "  Total Seq Acc: 36.574%\n",
      "    Avg Seq Acc: 87.146%\n",
      "Epoch: 17, Accuracy: 31.80%\n",
      "  Loss: 25%: 0.486688 50%: 0.593760 75%: 0.741733\n",
      "   Avg: 0.647466\n",
      "  Total Seq Acc: 38.834%\n",
      "    Avg Seq Acc: 87.917%\n",
      "Epoch: 18, Accuracy: 33.24%\n",
      "  Loss: 25%: 0.436313 50%: 0.527889 75%: 0.659536\n",
      "   Avg: 0.579617\n",
      "  Total Seq Acc: 39.065%\n",
      "    Avg Seq Acc: 88.180%\n",
      "Epoch: 19, Accuracy: 34.66%\n",
      "  Loss: 25%: 0.394753 50%: 0.478254 75%: 0.594729\n",
      "   Avg: 0.527243\n",
      "  Total Seq Acc: 39.577%\n",
      "    Avg Seq Acc: 88.354%\n",
      "Epoch: 20, Accuracy: 35.63%\n",
      "  Loss: 25%: 0.367282 50%: 0.446331 75%: 0.552213\n",
      "   Avg: 0.491565\n",
      "  Total Seq Acc: 39.801%\n",
      "    Avg Seq Acc: 88.395%\n",
      "Epoch: 21, Accuracy: 36.52%\n",
      "  Loss: 25%: 0.344380 50%: 0.417024 75%: 0.515350\n",
      "   Avg: 0.460924\n",
      "  Total Seq Acc: 39.923%\n",
      "    Avg Seq Acc: 88.536%\n",
      "Epoch: 22, Accuracy: 37.15%\n",
      "  Loss: 25%: 0.327414 50%: 0.395074 75%: 0.487842\n",
      "   Avg: 0.439581\n",
      "  Total Seq Acc: 40.359%\n",
      "    Avg Seq Acc: 88.574%\n",
      "Epoch: 23, Accuracy: 37.71%\n",
      "  Loss: 25%: 0.313323 50%: 0.379837 75%: 0.466719\n",
      "   Avg: 0.421587\n",
      "  Total Seq Acc: 40.602%\n",
      "    Avg Seq Acc: 88.734%\n",
      "Epoch: 24, Accuracy: 38.17%\n",
      "  Loss: 25%: 0.303001 50%: 0.364846 75%: 0.447742\n",
      "   Avg: 0.403929\n",
      "  Total Seq Acc: 40.647%\n",
      "    Avg Seq Acc: 88.777%\n",
      "Epoch: 25, Accuracy: 38.67%\n",
      "  Loss: 25%: 0.290715 50%: 0.351060 75%: 0.427329\n",
      "   Avg: 0.386927\n",
      "  Total Seq Acc: 40.608%\n",
      "    Avg Seq Acc: 88.742%\n",
      "Epoch: 26, Accuracy: 38.95%\n",
      "  Loss: 25%: 0.282604 50%: 0.339503 75%: 0.412173\n",
      "   Avg: 0.375377\n",
      "  Total Seq Acc: 40.890%\n",
      "    Avg Seq Acc: 88.844%\n",
      "Epoch: 27, Accuracy: 39.26%\n",
      "  Loss: 25%: 0.276607 50%: 0.331959 75%: 0.404896\n",
      "   Avg: 0.368596\n",
      "  Total Seq Acc: 40.788%\n",
      "    Avg Seq Acc: 88.860%\n",
      "Epoch: 28, Accuracy: 39.62%\n",
      "  Loss: 25%: 0.269253 50%: 0.322100 75%: 0.391046\n",
      "   Avg: 0.357316\n",
      "  Total Seq Acc: 41.114%\n",
      "    Avg Seq Acc: 88.998%\n",
      "Epoch: 29, Accuracy: 39.94%\n",
      "  Loss: 25%: 0.262997 50%: 0.317211 75%: 0.381432\n",
      "   Avg: 0.349266\n",
      "  Total Seq Acc: 41.223%\n",
      "    Avg Seq Acc: 89.001%\n",
      "Epoch: 30, Accuracy: 40.06%\n",
      "  Loss: 25%: 0.258675 50%: 0.309745 75%: 0.373555\n",
      "   Avg: 0.341003\n",
      "  Total Seq Acc: 41.326%\n",
      "    Avg Seq Acc: 88.976%\n",
      "Epoch: 31, Accuracy: 40.41%\n",
      "  Loss: 25%: 0.251329 50%: 0.302622 75%: 0.366014\n",
      "   Avg: 0.333494\n",
      "  Total Seq Acc: 41.505%\n",
      "    Avg Seq Acc: 89.060%\n",
      "Epoch: 32, Accuracy: 40.68%\n",
      "  Loss: 25%: 0.247923 50%: 0.298805 75%: 0.361045\n",
      "   Avg: 0.330150\n",
      "  Total Seq Acc: 41.076%\n",
      "    Avg Seq Acc: 88.967%\n",
      "Epoch: 33, Accuracy: 40.69%\n",
      "  Loss: 25%: 0.247097 50%: 0.295387 75%: 0.355061\n",
      "   Avg: 0.324294\n",
      "  Total Seq Acc: 41.089%\n",
      "    Avg Seq Acc: 88.985%\n",
      "Epoch: 34, Accuracy: 40.82%\n",
      "  Loss: 25%: 0.242226 50%: 0.290121 75%: 0.348251\n",
      "   Avg: 0.319312\n",
      "  Total Seq Acc: 41.415%\n",
      "    Avg Seq Acc: 89.027%\n",
      "Epoch: 35, Accuracy: 41.01%\n",
      "  Loss: 25%: 0.237273 50%: 0.284986 75%: 0.342848\n",
      "   Avg: 0.313189\n",
      "  Total Seq Acc: 41.102%\n",
      "    Avg Seq Acc: 89.015%\n",
      "> KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.9})\n",
    "trainFor(50, target_acc=0.8)\n",
    "# (583mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I accidentally set the `target_acc` which is accuracy of total sequences, instead of `target_seq_acc` which is the average accuracy of sequences, so this training stage had to be manually stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "  Total Seq Acc: 52.339%\n",
      "    Avg Seq Acc: 90.122%\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 78.132%\n",
      "    Avg Seq Acc: 92.685%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 41.089%\n",
      "    Avg Seq Acc: 89.000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8899978608990418, 0.41088696765930194)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 140528\n",
      "  Total: 147337\n",
      "Epoch: 36, Accuracy: 41.15%10525, loss: 0.603985      \n",
      "  Loss: 25%: 0.235659 50%: 0.282068 75%: 0.340233\n",
      "   Avg: 0.309650\n",
      "  Total Seq Acc: 52.727%\n",
      "    Avg Seq Acc: 90.170%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.9})\n",
    "trainFor(50, target_p50_loss=0.2, target_seq_acc=0.90)\n",
    "# (31mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "  Total Seq Acc: 52.727%\n",
      "    Avg Seq Acc: 90.170%\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 78.249%\n",
      "    Avg Seq Acc: 92.715%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 41.595%\n",
      "    Avg Seq Acc: 89.056%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8905585711635232, 0.4159462055715658)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 10\n",
      "  Total: 6819\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 90\n",
      "  Total: 6899\n",
      "Epoch: 37, Accuracy: 72.87%                                             \n",
      "  Loss: 25%: 0.206330 50%: 0.297163 75%: 0.455768\n",
      "   Avg: 1.136580\n",
      "  Total Seq Acc: 78.267%\n",
      "    Avg Seq Acc: 92.671%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(3, target_p50_loss=0.2, target_seq_acc=0.90)\n",
    "# (mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - grammar: 10\n",
      "  Total: 11\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 90\n",
      "  Total: 6899\n",
      "Epoch: 38, Accuracy: 73.69%                                             \n",
      "  Loss: 25%: 0.211418 50%: 0.300939 75%: 0.432739\n",
      "   Avg: 0.994158\n",
      "  Total Seq Acc: 0.000%\n",
      "    Avg Seq Acc: 19.835%\n",
      "Epoch: 39, Accuracy: 74.16%                                             \n",
      "  Loss: 25%: 0.214725 50%: 0.280471 75%: 0.424626\n",
      "   Avg: 0.985662\n",
      "  Total Seq Acc: 0.000%\n",
      "    Avg Seq Acc: 19.056%\n",
      "Epoch: 40, Accuracy: 73.60%                                             \n",
      "  Loss: 25%: 0.204188 50%: 0.284730 75%: 0.447475\n",
      "   Avg: 0.963902\n",
      "  Total Seq Acc: 0.000%\n",
      "    Avg Seq Acc: 20.938%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(3, target_p50_loss=0.2, target_seq_acc=0.90)\n",
    "# (mins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
