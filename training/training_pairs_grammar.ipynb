{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded epoch_55\n"
     ]
    }
   ],
   "source": [
    "# Init/Load model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Define a directory to save the models\n",
    "SAVE_DIR = '../saved_models'\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "start_epoch = 55\n",
    "\n",
    "class SimpleBART(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleBART, self).__init__()\n",
    "        if start_epoch > 0:\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained(os.path.join(SAVE_DIR, f'epoch_{start_epoch}'))\n",
    "            print(f'Loaded epoch_{start_epoch}')\n",
    "        else:\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "            print('Loaded facebook/bart-base')\n",
    "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "model = SimpleBART().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Raw Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "\n",
    "ACCUMULATION_STEPS = 14\n",
    "BATCH_SIZE = 14 # best performing batch size so far (in execution performance)\n",
    "DATA_SIZE = 0\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=200):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.start_token_id = tokenizer.cls_token_id\n",
    "        self.end_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, tokens = self.data[idx]\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
    "\n",
    "        # Add start and end tokens and then pad\n",
    "        tokens = [self.start_token_id] + tokens + [self.end_token_id]\n",
    "        tokens_padded = [self.pad_token_id] * self.max_length\n",
    "        tokens_padded[:len(tokens)] = tokens\n",
    "        tokens_padded[len(tokens):] = [self.pad_token_id] * (self.max_length - len(tokens))\n",
    "\n",
    "        return inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), torch.tensor(tokens_padded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def load_data_from_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [(row[0], [int(tok) for tok in row[1].split(\",\")]) for row in reader]\n",
    "\n",
    "    return data\n",
    "\n",
    "def apply_concept(params, validation=False):\n",
    "    global validationLoader\n",
    "    global dataloader\n",
    "    global DATA_SIZE\n",
    "\n",
    "    merged_data = load_data_from_csv(f\"../concept/egg.csv\")\n",
    "\n",
    "    print(\"Concepts loaded;\")\n",
    "    for file_name, percentage in params.items():\n",
    "        data = load_data_from_csv(f\"../concept/{file_name}.csv\")\n",
    "        cutoff = int(len(data) * percentage)\n",
    "\n",
    "        if validation:\n",
    "            loaded_data = data[-cutoff:]\n",
    "        else:\n",
    "            loaded_data = data[:cutoff]\n",
    "\n",
    "        print(f\"    - {file_name}: {len(loaded_data)}\")\n",
    "        merged_data.extend(loaded_data)\n",
    "\n",
    "    DATA_SIZE = len(merged_data)\n",
    "    dataset = CustomDataset(merged_data, model.tokenizer)\n",
    "    if validation:\n",
    "        validationLoader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(f'  Total: {DATA_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Validator\n",
    "def validate():\n",
    "    EOS_TOKEN_ID = model.tokenizer.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for accuracy calculation\n",
    "    total_correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "\n",
    "    # Initialize counters for average sequence accuracy within the mask\n",
    "    total_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask, targets) in enumerate(validationLoader):\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Identify where the EOS token is in the target sequence\n",
    "            eos_positions = (targets == EOS_TOKEN_ID).cumsum(dim=1).type(torch.bool)\n",
    "            mask = ~eos_positions | (targets == EOS_TOKEN_ID)\n",
    "\n",
    "            _, predicted = logits.max(2)\n",
    "            correct_sequences = ((predicted == targets) | ~mask).all(dim=1).float().sum().item()\n",
    "            total_sequences += targets.size(0)\n",
    "            total_correct_sequences += correct_sequences\n",
    "\n",
    "            # Compute the accuracy for each sequence\n",
    "            correct_tokens_per_sequence = ((predicted == targets) & mask).float().sum(dim=1)\n",
    "            total_tokens_per_sequence = mask.float().sum(dim=1)\n",
    "            total_accuracy += (correct_tokens_per_sequence / total_tokens_per_sequence).sum().item()\n",
    "\n",
    "    # Compute and print the accuracy for the entire validation dataset\n",
    "    validation_accuracy = total_correct_sequences / total_sequences\n",
    "    print(f\"  Total Seq Acc: {validation_accuracy*100:.3f}%\")\n",
    "    avg_accuracy = total_accuracy / total_sequences\n",
    "    print(f\"    Avg Seq Acc: {avg_accuracy*100:.3f}%\")\n",
    "\n",
    "    return avg_accuracy, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "def save_model():\n",
    "    global start_epoch\n",
    "    model_save_path = os.path.join(SAVE_DIR, f'epoch_{start_epoch}')\n",
    "    model.bart.save_pretrained(model_save_path)\n",
    "\n",
    "def trainFor(num_epochs, target_loss=0, target_p50_loss=0, target_acc=1.0, target_seq_acc=1.0):\n",
    "    global start_epoch\n",
    "\n",
    "    EOS_TOKEN_ID = model.tokenizer.eos_token_id\n",
    "    acc_batch = int(ACCUMULATION_STEPS / BATCH_SIZE)\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for epoch in range(start_epoch+1, start_epoch+num_epochs+1):\n",
    "        start_epoch = epoch\n",
    "        model.train()\n",
    "\n",
    "        # Resetting the accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize counters for accuracy calculation\n",
    "        total_correct_sequences = 0\n",
    "        total_sequences = 0\n",
    "        cumulative_loss = 0.0\n",
    "\n",
    "        # Initialize list to store batch losses\n",
    "        batch_losses = []\n",
    "\n",
    "        for batch_idx, (input_ids, attention_mask, targets) in enumerate(dataloader):\n",
    "\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Identify where the EOS token is in the target sequence\n",
    "            eos_positions = (targets == EOS_TOKEN_ID).cumsum(dim=1).type(torch.bool)\n",
    "            mask = ~eos_positions | (targets == EOS_TOKEN_ID)\n",
    "\n",
    "            # Apply mask to filter out tokens after the EOS token for loss computation\n",
    "            active_loss = mask.view(-1).bool()\n",
    "            active_logits = logits.view(-1, logits.size(-1))[active_loss]\n",
    "            active_labels = targets.view(-1)[active_loss]\n",
    "            loss = criterion(active_logits, active_labels)\n",
    "\n",
    "            _, predicted = logits.max(2)\n",
    "            correct_sequences = ((predicted == targets) | ~mask).all(dim=1).float().sum().item()\n",
    "            total_sequences += targets.size(0)\n",
    "            total_correct_sequences += correct_sequences\n",
    "\n",
    "            # Accumulate the gradients\n",
    "            loss.backward()\n",
    "            loss_val = loss.item()\n",
    "\n",
    "            cumulative_loss += loss_val\n",
    "            batch_losses.append(loss_val)\n",
    "\n",
    "            isLast = batch_idx == len(dataloader) - 1\n",
    "\n",
    "            # Only perform an optimization step every ACCUMULATION_STEPS\n",
    "            if isLast or batch_idx % acc_batch == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            print(f\"\\rEpoch: {epoch}, Batch: {batch_idx} of {total_batches}, loss: {loss_val:.6f}      \", end='')\n",
    "\n",
    "\n",
    "        # Compute and print the accuracy for the entire epoch\n",
    "        epoch_accuracy = total_correct_sequences / total_sequences\n",
    "        cumulative_loss = cumulative_loss / len(batch_losses)\n",
    "        p25_loss = np.percentile(batch_losses, 25)\n",
    "        p50_loss = np.percentile(batch_losses, 50)\n",
    "        p75_loss = np.percentile(batch_losses, 75)\n",
    "        print(f\"\\rEpoch: {epoch}, Accuracy: {epoch_accuracy*100:.2f}%                                             \")\n",
    "        print(f\"  Loss: 25%: {p25_loss:.6f} 50%: {p50_loss:.6f} 75%: {p75_loss:.6f}\\n   Avg: {cumulative_loss:.6f}\")\n",
    "\n",
    "        # Write loss values to loss.csv\n",
    "        percentiles = [np.percentile(batch_losses, i) for i in range(101)]\n",
    "        with open('loss.csv', mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch] + percentiles)\n",
    "\n",
    "        if epoch % 10 == 0: # Save the model\n",
    "            save_model()\n",
    "\n",
    "        seq_acc, total_acc = validate()\n",
    "\n",
    "        if total_acc >= target_acc:\n",
    "            break\n",
    "        if cumulative_loss <= target_loss:\n",
    "            break\n",
    "        if p50_loss <= target_p50_loss:\n",
    "            break\n",
    "\n",
    "        if seq_acc >= target_seq_acc:\n",
    "            break\n",
    "\n",
    "    # Make sure last epoch is always saved\n",
    "    save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 3122\n",
      "  Total: 9931\n",
      "Epoch: 1, Accuracy: 0.00%                                             \n",
      "  Loss: 25%: 4.403065 50%: 4.849018 75%: 5.296900\n",
      "   Avg: 4.866282\n",
      "  Total Seq Acc: 0.004%\n",
      "    Avg Seq Acc: 39.869%\n",
      "Epoch: 2, Accuracy: 0.04%                                             \n",
      "  Loss: 25%: 4.114165 50%: 4.543126 75%: 4.914825\n",
      "   Avg: 4.488807\n",
      "  Total Seq Acc: 0.000%\n",
      "    Avg Seq Acc: 39.873%\n",
      "Epoch: 3, Accuracy: 0.04%                                             \n",
      "  Loss: 25%: 4.032996 50%: 4.430487 75%: 4.820580\n",
      "   Avg: 4.396551\n",
      "  Total Seq Acc: 0.076%\n",
      "    Avg Seq Acc: 39.906%\n",
      "Epoch: 4, Accuracy: 0.06%                                             \n",
      "  Loss: 25%: 3.972137 50%: 4.378263 75%: 4.781254\n",
      "   Avg: 4.345113\n",
      "  Total Seq Acc: 0.120%\n",
      "    Avg Seq Acc: 39.934%\n",
      "Epoch: 5, Accuracy: 0.12%                                             \n",
      "  Loss: 25%: 3.932351 50%: 4.367048 75%: 4.685833\n",
      "   Avg: 4.293642\n",
      "  Total Seq Acc: 0.299%\n",
      "    Avg Seq Acc: 40.057%\n",
      "Epoch: 6, Accuracy: 0.37%                                             \n",
      "  Loss: 25%: 3.837589 50%: 4.228431 75%: 4.601477\n",
      "   Avg: 4.183411\n",
      "  Total Seq Acc: 0.945%\n",
      "    Avg Seq Acc: 40.459%\n",
      "Epoch: 7, Accuracy: 1.64%                                             \n",
      "  Loss: 25%: 3.571904 50%: 4.009486 75%: 4.325815\n",
      "   Avg: 3.957460\n",
      "  Total Seq Acc: 3.233%\n",
      "    Avg Seq Acc: 42.656%\n",
      "Epoch: 8, Accuracy: 5.18%                                             \n",
      "  Loss: 25%: 3.171680 50%: 3.612419 75%: 3.961515\n",
      "   Avg: 3.558381\n",
      "  Total Seq Acc: 7.301%\n",
      "    Avg Seq Acc: 48.088%\n",
      "Epoch: 9, Accuracy: 10.70%                                             \n",
      "  Loss: 25%: 2.747878 50%: 3.077505 75%: 3.433291\n",
      "   Avg: 3.070574\n",
      "  Total Seq Acc: 12.099%\n",
      "    Avg Seq Acc: 55.879%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = BATCH_SIZE # pure memorisation so accumulation won't help\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.02})\n",
    "trainFor(50, target_seq_acc=0.50)\n",
    "# (34mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 6245\n",
      "  Total: 13054\n",
      "Epoch: 10, Accuracy: 13.81%                                             \n",
      "  Loss: 25%: 2.656691 50%: 2.988907 75%: 3.271646\n",
      "   Avg: 2.973416\n",
      "  Total Seq Acc: 19.721%\n",
      "    Avg Seq Acc: 68.222%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = BATCH_SIZE # pure memorisation so accumulation won't help\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.04})\n",
    "trainFor(50, target_seq_acc=0.60)\n",
    "# (5mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 9368\n",
      "  Total: 16177\n",
      "Epoch: 11, Accuracy: 17.17%                                             \n",
      "  Loss: 25%: 2.074187 50%: 2.373098 75%: 2.669538\n",
      "   Avg: 2.376195\n",
      "  Total Seq Acc: 27.387%\n",
      "    Avg Seq Acc: 76.537%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 28 # *14 close to 32\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.06})\n",
    "trainFor(50, target_seq_acc=0.65)\n",
    "# (5mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aiming for:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{unique}{tokens} &= \\frac{4174}{6808} = 61.31\\%  & \\text{sign pairs to text only used once} \\\\\n",
    "  \\frac{text}{tokens}   &= \\frac{5342}{6808}  = 78.47\\% & \\text{sign pairs to text unique text} \\\\\n",
    "  & & \\text{unique meaning the text is only used for one tokenID}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "i.e. there are six different signs which can be used for \"present\"  \n",
    "which means we're actually aiming for $96\\%$ effective accuracy $\\frac{75\\%}{78\\%}$\n",
    "\n",
    "But we don't want to over-fit either  \n",
    "Hence why we slowly introduce new concepts while still memorising vocabulary\n",
    "\n",
    "`target_seq_acc` includes the `EOS` token, which we of course we want to be right\n",
    "So our target should be $\\frac{61.31\\% + 100\\%}{2} = 80.65\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 12491\n",
      "  Total: 19300\n",
      "Epoch: 12, Accuracy: 20.01%                                             \n",
      "  Loss: 25%: 1.637285 50%: 1.861813 75%: 2.106627\n",
      "   Avg: 1.887390\n",
      "  Total Seq Acc: 32.244%\n",
      "    Avg Seq Acc: 80.065%\n",
      "Epoch: 13, Accuracy: 23.50%                                             \n",
      "  Loss: 25%: 1.358196 50%: 1.585695 75%: 1.840750\n",
      "   Avg: 1.606165\n",
      "  Total Seq Acc: 34.451%\n",
      "    Avg Seq Acc: 81.583%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 70 # *14 close to 64\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.08})\n",
    "trainFor(50, target_seq_acc=0.80655)\n",
    "# (11mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure memorisation is over, as the vocabulary has been sufficiently learnt  \n",
    "While it's not perfect, it will improve further over the later training, as the vocab remains in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 35132\n",
      "  Total: 41941\n",
      "Epoch: 14, Accuracy: 18.78%                                             \n",
      "  Loss: 25%: 1.329572 50%: 1.558345 75%: 1.814157\n",
      "   Avg: 1.585541\n",
      "  Total Seq Acc: 37.921%\n",
      "    Avg Seq Acc: 83.464%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.225})\n",
    "trainFor(50, target_seq_acc=0.6)\n",
    "# (10mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 70264\n",
      "  Total: 77073\n",
      "Epoch: 15, Accuracy: 20.15%                                             \n",
      "  Loss: 25%: 1.079807 50%: 1.281758 75%: 1.525836\n",
      "   Avg: 1.319893\n",
      "  Total Seq Acc: 41.399%\n",
      "    Avg Seq Acc: 85.418%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.45})\n",
    "trainFor(50, target_seq_acc=0.7)\n",
    "# (16mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 140528\n",
      "  Total: 147337\n",
      "Epoch: 16, Accuracy: 23.13%                                             \n",
      "  Loss: 25%: 0.838045 50%: 1.006275 75%: 1.225789\n",
      "   Avg: 1.055101\n",
      "  Total Seq Acc: 44.762%\n",
      "    Avg Seq Acc: 87.021%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.9})\n",
    "trainFor(50, target_seq_acc=0.75)\n",
    "# (30mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 140528\n",
      "  Total: 147337\n",
      "Epoch: 17, Accuracy: 26.92%                                             \n",
      "  Loss: 25%: 0.662440 50%: 0.804522 75%: 0.992638\n",
      "   Avg: 0.856268\n",
      "  Total Seq Acc: 45.779%\n",
      "    Avg Seq Acc: 87.521%\n",
      "Epoch: 18, Accuracy: 29.49%                                             \n",
      "  Loss: 25%: 0.562557 50%: 0.685241 75%: 0.851517\n",
      "   Avg: 0.737888\n",
      "  Total Seq Acc: 47.928%\n",
      "    Avg Seq Acc: 88.370%\n",
      "Epoch: 19, Accuracy: 31.52%                                             \n",
      "  Loss: 25%: 0.491418 50%: 0.597656 75%: 0.744295\n",
      "   Avg: 0.650201\n",
      "  Total Seq Acc: 48.513%\n",
      "    Avg Seq Acc: 88.522%\n",
      "Epoch: 20, Accuracy: 32.73%                                             \n",
      "  Loss: 25%: 0.447802 50%: 0.545643 75%: 0.681414\n",
      "   Avg: 0.597653\n",
      "  Total Seq Acc: 49.436%\n",
      "    Avg Seq Acc: 88.904%\n",
      "Epoch: 21, Accuracy: 34.06%                                             \n",
      "  Loss: 25%: 0.411880 50%: 0.501514 75%: 0.623499\n",
      "   Avg: 0.550958\n",
      "  Total Seq Acc: 49.931%\n",
      "    Avg Seq Acc: 89.166%\n",
      "Epoch: 22, Accuracy: 35.01%                                             \n",
      "  Loss: 25%: 0.384031 50%: 0.466311 75%: 0.577149\n",
      "   Avg: 0.513654\n",
      "  Total Seq Acc: 50.270%\n",
      "    Avg Seq Acc: 89.304%\n",
      "Epoch: 23, Accuracy: 35.84%                                             \n",
      "  Loss: 25%: 0.362658 50%: 0.438559 75%: 0.541728\n",
      "   Avg: 0.483325\n",
      "  Total Seq Acc: 50.894%\n",
      "    Avg Seq Acc: 89.507%\n",
      "Epoch: 24, Accuracy: 36.37%                                             \n",
      "  Loss: 25%: 0.343976 50%: 0.415291 75%: 0.513873\n",
      "   Avg: 0.460318\n",
      "  Total Seq Acc: 50.912%\n",
      "    Avg Seq Acc: 89.472%\n",
      "Epoch: 25, Accuracy: 37.01%                                             \n",
      "  Loss: 25%: 0.332605 50%: 0.403996 75%: 0.502858\n",
      "   Avg: 0.449104\n",
      "  Total Seq Acc: 51.041%\n",
      "    Avg Seq Acc: 89.575%\n",
      "Epoch: 26, Accuracy: 37.62%                                             \n",
      "  Loss: 25%: 0.317929 50%: 0.386799 75%: 0.481079\n",
      "   Avg: 0.429650\n",
      "  Total Seq Acc: 51.197%\n",
      "    Avg Seq Acc: 89.643%\n",
      "Epoch: 27, Accuracy: 37.94%                                             \n",
      "  Loss: 25%: 0.308349 50%: 0.372526 75%: 0.456689\n",
      "   Avg: 0.413500\n",
      "  Total Seq Acc: 51.577%\n",
      "    Avg Seq Acc: 89.717%\n",
      "Epoch: 28, Accuracy: 38.33%                                             \n",
      "  Loss: 25%: 0.296087 50%: 0.356619 75%: 0.437291\n",
      "   Avg: 0.396411\n",
      "  Total Seq Acc: 51.643%\n",
      "    Avg Seq Acc: 89.830%\n",
      "Epoch: 29, Accuracy: 38.65%                                             \n",
      "  Loss: 25%: 0.289713 50%: 0.347075 75%: 0.422274\n",
      "   Avg: 0.384405\n",
      "  Total Seq Acc: 51.648%\n",
      "    Avg Seq Acc: 89.834%\n",
      "Epoch: 30, Accuracy: 39.05%                                             \n",
      "  Loss: 25%: 0.279989 50%: 0.338640 75%: 0.412372\n",
      "   Avg: 0.374840\n",
      "  Total Seq Acc: 52.192%\n",
      "    Avg Seq Acc: 89.935%\n",
      "Epoch: 31, Accuracy: 39.41%                                             \n",
      "  Loss: 25%: 0.273525 50%: 0.329196 75%: 0.399910\n",
      "   Avg: 0.365167\n",
      "  Total Seq Acc: 52.089%\n",
      "    Avg Seq Acc: 89.978%\n",
      "Epoch: 32, Accuracy: 39.70%                                             \n",
      "  Loss: 25%: 0.267469 50%: 0.320570 75%: 0.389043\n",
      "   Avg: 0.355207\n",
      "  Total Seq Acc: 51.960%\n",
      "    Avg Seq Acc: 89.957%\n",
      "Epoch: 33, Accuracy: 39.86%                                             \n",
      "  Loss: 25%: 0.264502 50%: 0.317547 75%: 0.383463\n",
      "   Avg: 0.350738\n",
      "  Total Seq Acc: 51.880%\n",
      "    Avg Seq Acc: 89.958%\n",
      "Epoch: 34, Accuracy: 40.03%                                             \n",
      "  Loss: 25%: 0.258477 50%: 0.310631 75%: 0.377177\n",
      "   Avg: 0.342286\n",
      "  Total Seq Acc: 52.165%\n",
      "    Avg Seq Acc: 90.004%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.9})\n",
    "trainFor(50, target_p50_loss=0.2, target_seq_acc=0.90)\n",
    "# (533mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "  Total Seq Acc: 52.165%\n",
      "    Avg Seq Acc: 90.004%\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 77.486%\n",
      "    Avg Seq Acc: 92.470%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 41.121%\n",
      "    Avg Seq Acc: 88.925%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.88925186383812, 0.4112071725904579)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has successfully applied a vocabulary swap we must start the grammatical transformation training.  \n",
    "First we will start by just teaching it which words to ignore as they do not exist in Auslan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 10000\n",
      "  Total: 16809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 45000\n",
      "  Total: 51809\n",
      "Epoch: 35, Accuracy: 9.80%                                             \n",
      "  Loss: 25%: 5.300752 50%: 5.609476 75%: 5.946348\n",
      "   Avg: 5.744106\n",
      "  Total Seq Acc: 31.804%\n",
      "    Avg Seq Acc: 58.800%\n",
      "Epoch: 36, Accuracy: 9.81%                                             \n",
      "  Loss: 25%: 4.243643 50%: 4.516359 75%: 4.793437\n",
      "   Avg: 4.508092\n",
      "  Total Seq Acc: 31.638%\n",
      "    Avg Seq Acc: 64.020%\n",
      "Epoch: 37, Accuracy: 9.89%                                             \n",
      "  Loss: 25%: 3.599592 50%: 3.846125 75%: 4.072579\n",
      "   Avg: 3.822467\n",
      "  Total Seq Acc: 32.019%\n",
      "    Avg Seq Acc: 64.472%\n",
      "Epoch: 38, Accuracy: 10.03%                                             \n",
      "  Loss: 25%: 3.236534 50%: 3.473460 75%: 3.700928\n",
      "   Avg: 3.461110\n",
      "  Total Seq Acc: 31.751%\n",
      "    Avg Seq Acc: 65.270%\n",
      "Epoch: 39, Accuracy: 10.03%                                             \n",
      "  Loss: 25%: 3.030043 50%: 3.260919 75%: 3.484393\n",
      "   Avg: 3.248439\n",
      "  Total Seq Acc: 32.120%\n",
      "    Avg Seq Acc: 71.423%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 4088 # *14 close to 4096\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.45})\n",
    "trainFor(20, target_p50_loss=0.2, target_seq_acc=0.70)\n",
    "# (57mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 10000\n",
      "  Total: 16809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 90000\n",
      "  Total: 96809\n",
      "Epoch: 40, Accuracy: 5.95%                                             \n",
      "  Loss: 25%: 2.783838 50%: 3.019561 75%: 3.248209\n",
      "   Avg: 3.012249\n",
      "  Total Seq Acc: 31.828%\n",
      "    Avg Seq Acc: 69.566%\n",
      "Epoch: 41, Accuracy: 6.10%                                             \n",
      "  Loss: 25%: 2.493118 50%: 2.705811 75%: 2.920518\n",
      "   Avg: 2.705100\n",
      "  Total Seq Acc: 32.019%\n",
      "    Avg Seq Acc: 72.888%\n",
      "Epoch: 42, Accuracy: 6.09%                                             \n",
      "  Loss: 25%: 2.316473 50%: 2.525594 75%: 2.723895\n",
      "   Avg: 2.516568\n",
      "  Total Seq Acc: 32.310%\n",
      "    Avg Seq Acc: 76.054%\n",
      "Epoch: 43, Accuracy: 6.19%                                             \n",
      "  Loss: 25%: 2.155381 50%: 2.360453 75%: 2.556274\n",
      "   Avg: 2.357171\n",
      "  Total Seq Acc: 32.013%\n",
      "    Avg Seq Acc: 75.060%\n",
      "Epoch: 44, Accuracy: 6.24%                                             \n",
      "  Loss: 25%: 2.032776 50%: 2.226068 75%: 2.420145\n",
      "   Avg: 2.224036\n",
      "  Total Seq Acc: 32.132%\n",
      "    Avg Seq Acc: 77.114%\n",
      "> KeyboardInterrupt"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 4088 # *14 close to 4096\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.9})\n",
    "trainFor(20, target_p50_loss=0.2, target_seq_acc=0.9)\n",
    "# (mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancelled early because we don't want to put too much focus into just the word masking  \n",
    "The model already has a strong grasp on the concept and can start learning word arranging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-pairs: 10400\n",
      "  Total: 17209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-pairs: 6240\n",
      "  Total: 13049\n",
      "Epoch: 45, Accuracy: 57.50%                                             \n",
      "  Loss: 25%: 0.770977 50%: 1.173722 75%: 1.598817\n",
      "   Avg: 1.226389\n",
      "  Total Seq Acc: 61.863%\n",
      "    Avg Seq Acc: 85.398%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-pairs\": 0.06})\n",
    "trainFor(1, target_p50_loss=2.0, target_seq_acc=0.4)\n",
    "# (4mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-pairs: 10400\n",
      "  Total: 17209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-pairs: 10400\n",
      "  Total: 17209\n",
      "Epoch: 46, Accuracy: 52.90%                                             \n",
      "  Loss: 25%: 0.827584 50%: 1.168207 75%: 1.525043\n",
      "   Avg: 1.209725\n",
      "  Total Seq Acc: 60.021%\n",
      "    Avg Seq Acc: 85.148%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-pairs\": 0.1})\n",
    "trainFor(1, target_p50_loss=2.0, target_seq_acc=0.4)\n",
    "# (5mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-pairs: 10400\n",
      "  Total: 17209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-pairs: 46800\n",
      "  Total: 53609\n",
      "Epoch: 47, Accuracy: 39.09%                                             \n",
      "  Loss: 25%: 0.956436 50%: 1.164306 75%: 1.400537\n",
      "   Avg: 1.192730\n",
      "  Total Seq Acc: 54.965%\n",
      "    Avg Seq Acc: 83.826%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-pairs\": 0.45})\n",
    "trainFor(2, target_p50_loss=2.0, target_seq_acc=0.4)\n",
    "# (12mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "    - word-pairs: 10400\n",
      "  Total: 67209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 5000\n",
      "    - word-pairs: 93600\n",
      "  Total: 105409\n",
      "Epoch: 48, Accuracy: 35.26%                                             \n",
      "  Loss: 25%: 1.029486 50%: 1.585286 75%: 3.445197\n",
      "   Avg: 2.245614\n",
      "  Total Seq Acc: 14.166%\n",
      "    Avg Seq Acc: 35.970%\n",
      "Epoch: 49, Accuracy: 38.52%                                             \n",
      "  Loss: 25%: 0.895498 50%: 1.427730 75%: 3.029519\n",
      "   Avg: 1.967145\n",
      "  Total Seq Acc: 15.263%\n",
      "    Avg Seq Acc: 37.225%\n",
      "Epoch: 50, Accuracy: 40.25%                                             \n",
      "  Loss: 25%: 0.770915 50%: 1.255461 75%: 2.870213\n",
      "   Avg: 1.825772\n",
      "  Total Seq Acc: 15.060%\n",
      "    Avg Seq Acc: 37.681%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.01, \"word-pairs\": 0.9})\n",
    "trainFor(3, target_p50_loss=1.0, target_seq_acc=0.4)\n",
    "# (75mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "    - word-pairs: 10400\n",
      "  Total: 67209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "    - word-pairs: 93600\n",
      "  Total: 550409\n",
      "Epoch: 51, Accuracy: 6.26%                                             \n",
      "  Loss: 25%: 4.416064 50%: 4.600059 75%: 4.796405\n",
      "   Avg: 4.610680\n",
      "  Total Seq Acc: 12.513%\n",
      "    Avg Seq Acc: 39.037%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9, \"word-pairs\": 0.9})\n",
    "trainFor(5, target_p50_loss=1.0, target_seq_acc=0.3)\n",
    "# (114mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "    - word-pairs: 10400\n",
      "  Total: 67209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "    - word-pairs: 93600\n",
      "  Total: 550409\n",
      "Epoch: 52, Accuracy: 5.56%                                             \n",
      "  Loss: 25%: 4.032571 50%: 4.196765 75%: 4.365591\n",
      "   Avg: 4.200637\n",
      "  Total Seq Acc: 11.856%\n",
      "    Avg Seq Acc: 40.983%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9, \"word-pairs\": 0.9})\n",
    "trainFor(3, target_p50_loss=1.5, target_seq_acc=0.8)\n",
    "# (114mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "    - word-pairs: 10400\n",
      "  Total: 67209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "    - word-pairs: 93600\n",
      "  Total: 550409\n",
      "Epoch: 53, Accuracy: 6.00%                                             \n",
      "  Loss: 25%: 3.714889 50%: 3.890947 75%: 4.075457\n",
      "   Avg: 3.898209\n",
      "  Total Seq Acc: 11.595%\n",
      "    Avg Seq Acc: 43.582%\n",
      "Epoch: 54, Accuracy: 6.33%                                             \n",
      "  Loss: 25%: 3.452131 50%: 3.632052 75%: 3.821001\n",
      "   Avg: 3.641159\n",
      "  Total Seq Acc: 12.750%\n",
      "    Avg Seq Acc: 46.063%\n",
      "Epoch: 55, Accuracy: 6.77%                                             \n",
      "  Loss: 25%: 3.258086 50%: 3.442890 75%: 3.641506\n",
      "   Avg: 3.452856\n",
      "  Total Seq Acc: 13.349%\n",
      "    Avg Seq Acc: 48.018%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9, \"word-pairs\": 0.9})\n",
    "trainFor(3, target_p50_loss=1.5, target_seq_acc=0.8)\n",
    "# (350mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "    - word-pairs: 10400\n",
      "  Total: 67209\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "    - word-pairs: 93600\n",
      "  Total: 550409\n",
      "Epoch: 56, Accuracy: 7.02%                                             \n",
      "  Loss: 25%: 3.151639 50%: 3.343688 75%: 3.546811\n",
      "   Avg: 3.354264\n",
      "  Total Seq Acc: 13.440%\n",
      "    Avg Seq Acc: 48.859%\n",
      "Epoch: 57, Accuracy: 7.16%                                             \n",
      "  Loss: 25%: 3.025592 50%: 3.215463 75%: 3.418622\n",
      "   Avg: 3.228399\n",
      "  Total Seq Acc: 13.675%\n",
      "    Avg Seq Acc: 50.155%\n",
      "Epoch: 58, Accuracy: 7.30%                                             \n",
      "  Loss: 25%: 2.901687 50%: 3.097794 75%: 3.305452\n",
      "   Avg: 3.109050\n",
      "  Total Seq Acc: 13.959%\n",
      "    Avg Seq Acc: 51.758%\n",
      "Epoch: 59, Accuracy: 7.53%                                             \n",
      "  Loss: 25%: 2.777439 50%: 2.974927 75%: 3.184613\n",
      "   Avg: 2.986968\n",
      "  Total Seq Acc: 14.226%\n",
      "    Avg Seq Acc: 53.113%\n",
      "Epoch: 60, Accuracy: 7.76%                                             \n",
      "  Loss: 25%: 2.656170 50%: 2.855350 75%: 3.068886\n",
      "   Avg: 2.869071\n",
      "  Total Seq Acc: 14.510%\n",
      "    Avg Seq Acc: 54.471%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9, \"word-pairs\": 0.9})\n",
    "trainFor(5, target_p50_loss=1.5, target_seq_acc=0.8)\n",
    "# (567mins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
