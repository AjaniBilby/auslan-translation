{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded epoch_105\n"
     ]
    }
   ],
   "source": [
    "# Init/Load model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Define a directory to save the models\n",
    "SAVE_DIR = '../saved_models'\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "start_epoch = 110\n",
    "\n",
    "class SimpleBART(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleBART, self).__init__()\n",
    "        if start_epoch > 0:\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained(os.path.join(SAVE_DIR, f'epoch_{start_epoch}'))\n",
    "            print(f'Loaded epoch_{start_epoch}')\n",
    "        else:\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "            print('Loaded facebook/bart-base')\n",
    "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "model = SimpleBART().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Raw Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "\n",
    "ACCUMULATION_STEPS = 14\n",
    "BATCH_SIZE = 14 # best performing batch size so far (in execution performance)\n",
    "DATA_SIZE = 0\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=200):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.start_token_id = tokenizer.cls_token_id\n",
    "        self.end_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, tokens = self.data[idx]\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
    "\n",
    "        # Add start and end tokens and then pad\n",
    "        tokens = [self.start_token_id] + tokens + [self.end_token_id]\n",
    "        tokens_padded = [self.pad_token_id] * self.max_length\n",
    "        tokens_padded[:len(tokens)] = tokens\n",
    "        tokens_padded[len(tokens):] = [self.pad_token_id] * (self.max_length - len(tokens))\n",
    "\n",
    "        return inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), torch.tensor(tokens_padded, dtype=torch.long)\n",
    "\n",
    "\n",
    "def load_data_from_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [(row[0], [int(tok) for tok in row[1].split(\",\")]) for row in reader]\n",
    "\n",
    "    return data\n",
    "\n",
    "def apply_concept(params, validation=False):\n",
    "    global validationLoader\n",
    "    global dataloader\n",
    "    global DATA_SIZE\n",
    "\n",
    "    merged_data = load_data_from_csv(f\"../concept/egg.csv\")\n",
    "\n",
    "    print(\"Concepts loaded;\")\n",
    "    for file_name, percentage in params.items():\n",
    "        data = load_data_from_csv(f\"../concept/{file_name}.csv\")\n",
    "        cutoff = int(len(data) * percentage)\n",
    "\n",
    "        if validation:\n",
    "            loaded_data = data[-cutoff:]\n",
    "        else:\n",
    "            loaded_data = data[:cutoff]\n",
    "\n",
    "        print(f\"    - {file_name}: {len(loaded_data)}\")\n",
    "        merged_data.extend(loaded_data)\n",
    "\n",
    "    DATA_SIZE = len(merged_data)\n",
    "    dataset = CustomDataset(merged_data, model.tokenizer)\n",
    "    if validation:\n",
    "        validationLoader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(f'  Total: {DATA_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Validator\n",
    "def validate():\n",
    "    EOS_TOKEN_ID = model.tokenizer.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for accuracy calculation\n",
    "    total_correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "\n",
    "    # Initialize counters for average sequence accuracy within the mask\n",
    "    total_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask, targets) in enumerate(validationLoader):\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Identify where the EOS token is in the target sequence\n",
    "            eos_positions = (targets == EOS_TOKEN_ID).cumsum(dim=1).type(torch.bool)\n",
    "            mask = ~eos_positions | (targets == EOS_TOKEN_ID)\n",
    "\n",
    "            _, predicted = logits.max(2)\n",
    "            correct_sequences = ((predicted == targets) | ~mask).all(dim=1).float().sum().item()\n",
    "            total_sequences += targets.size(0)\n",
    "            total_correct_sequences += correct_sequences\n",
    "\n",
    "            # Compute the accuracy for each sequence\n",
    "            correct_tokens_per_sequence = ((predicted == targets) & mask).float().sum(dim=1)\n",
    "            total_tokens_per_sequence = mask.float().sum(dim=1)\n",
    "            total_accuracy += (correct_tokens_per_sequence / total_tokens_per_sequence).sum().item()\n",
    "\n",
    "    # Compute and print the accuracy for the entire validation dataset\n",
    "    validation_accuracy = total_correct_sequences / total_sequences\n",
    "    print(f\"  Total Seq Acc: {validation_accuracy*100:.3f}%\")\n",
    "    avg_accuracy = total_accuracy / total_sequences\n",
    "    print(f\"    Avg Seq Acc: {avg_accuracy*100:.3f}%\")\n",
    "\n",
    "    return avg_accuracy, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "def save_model():\n",
    "    global start_epoch\n",
    "    model_save_path = os.path.join(SAVE_DIR, f'epoch_{start_epoch}')\n",
    "    model.bart.save_pretrained(model_save_path)\n",
    "\n",
    "def trainFor(num_epochs, target_loss=0, target_p50_loss=0, target_acc=1.0, target_seq_acc=1.0):\n",
    "    global start_epoch\n",
    "\n",
    "    EOS_TOKEN_ID = model.tokenizer.eos_token_id\n",
    "    acc_batch = int(ACCUMULATION_STEPS / BATCH_SIZE)\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for epoch in range(start_epoch+1, start_epoch+num_epochs+1):\n",
    "        start_epoch = epoch\n",
    "        model.train()\n",
    "\n",
    "        # Resetting the accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize counters for accuracy calculation\n",
    "        total_correct_sequences = 0\n",
    "        total_sequences = 0\n",
    "        cumulative_loss = 0.0\n",
    "\n",
    "        # Initialize list to store batch losses\n",
    "        batch_losses = []\n",
    "\n",
    "        for batch_idx, (input_ids, attention_mask, targets) in enumerate(dataloader):\n",
    "\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Identify where the EOS token is in the target sequence\n",
    "            eos_positions = (targets == EOS_TOKEN_ID).cumsum(dim=1).type(torch.bool)\n",
    "            mask = ~eos_positions | (targets == EOS_TOKEN_ID)\n",
    "\n",
    "            # Apply mask to filter out tokens after the EOS token for loss computation\n",
    "            active_loss = mask.view(-1).bool()\n",
    "            active_logits = logits.view(-1, logits.size(-1))[active_loss]\n",
    "            active_labels = targets.view(-1)[active_loss]\n",
    "            loss = criterion(active_logits, active_labels)\n",
    "\n",
    "            _, predicted = logits.max(2)\n",
    "            correct_sequences = ((predicted == targets) | ~mask).all(dim=1).float().sum().item()\n",
    "            total_sequences += targets.size(0)\n",
    "            total_correct_sequences += correct_sequences\n",
    "\n",
    "            # Accumulate the gradients\n",
    "            loss.backward()\n",
    "            loss_val = loss.item()\n",
    "\n",
    "            cumulative_loss += loss_val\n",
    "            batch_losses.append(loss_val)\n",
    "\n",
    "            isLast = batch_idx == len(dataloader) - 1\n",
    "\n",
    "            # Only perform an optimization step every ACCUMULATION_STEPS\n",
    "            if isLast or batch_idx % acc_batch == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            print(f\"\\rEpoch: {epoch}, Batch: {batch_idx} of {total_batches}, loss: {loss_val:.6f}      \", end='')\n",
    "\n",
    "\n",
    "        # Compute and print the accuracy for the entire epoch\n",
    "        epoch_accuracy = total_correct_sequences / total_sequences\n",
    "        cumulative_loss = cumulative_loss / len(batch_losses)\n",
    "        p25_loss = np.percentile(batch_losses, 25)\n",
    "        p50_loss = np.percentile(batch_losses, 50)\n",
    "        p75_loss = np.percentile(batch_losses, 75)\n",
    "        print(f\"\\rEpoch: {epoch}, Accuracy: {epoch_accuracy*100:.2f}%                                             \")\n",
    "        print(f\"  Loss: 25%: {p25_loss:.6f} 50%: {p50_loss:.6f} 75%: {p75_loss:.6f}\\n   Avg: {cumulative_loss:.6f}\")\n",
    "\n",
    "        # Write loss values to loss.csv\n",
    "        percentiles = [np.percentile(batch_losses, i) for i in range(101)]\n",
    "        with open('loss.csv', mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch] + percentiles)\n",
    "\n",
    "        if epoch % 10 == 0: # Save the model\n",
    "            save_model()\n",
    "\n",
    "        seq_acc, total_acc = validate()\n",
    "\n",
    "        if total_acc >= target_acc:\n",
    "            break\n",
    "        if cumulative_loss <= target_loss:\n",
    "            break\n",
    "        if p50_loss <= target_p50_loss:\n",
    "            break\n",
    "\n",
    "        if seq_acc >= target_seq_acc:\n",
    "            break\n",
    "\n",
    "    # Make sure last epoch is always saved\n",
    "    save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 3122\n",
      "  Total: 9931\n",
      "Epoch: 1, Accuracy: 0.00%                                             \n",
      "  Loss: 25%: 4.403065 50%: 4.849018 75%: 5.296900\n",
      "   Avg: 4.866282\n",
      "  Total Seq Acc: 0.004%\n",
      "    Avg Seq Acc: 39.869%\n",
      "Epoch: 2, Accuracy: 0.04%                                             \n",
      "  Loss: 25%: 4.114165 50%: 4.543126 75%: 4.914825\n",
      "   Avg: 4.488807\n",
      "  Total Seq Acc: 0.000%\n",
      "    Avg Seq Acc: 39.873%\n",
      "Epoch: 3, Accuracy: 0.04%                                             \n",
      "  Loss: 25%: 4.032996 50%: 4.430487 75%: 4.820580\n",
      "   Avg: 4.396551\n",
      "  Total Seq Acc: 0.076%\n",
      "    Avg Seq Acc: 39.906%\n",
      "Epoch: 4, Accuracy: 0.06%                                             \n",
      "  Loss: 25%: 3.972137 50%: 4.378263 75%: 4.781254\n",
      "   Avg: 4.345113\n",
      "  Total Seq Acc: 0.120%\n",
      "    Avg Seq Acc: 39.934%\n",
      "Epoch: 5, Accuracy: 0.12%                                             \n",
      "  Loss: 25%: 3.932351 50%: 4.367048 75%: 4.685833\n",
      "   Avg: 4.293642\n",
      "  Total Seq Acc: 0.299%\n",
      "    Avg Seq Acc: 40.057%\n",
      "Epoch: 6, Accuracy: 0.37%                                             \n",
      "  Loss: 25%: 3.837589 50%: 4.228431 75%: 4.601477\n",
      "   Avg: 4.183411\n",
      "  Total Seq Acc: 0.945%\n",
      "    Avg Seq Acc: 40.459%\n",
      "Epoch: 7, Accuracy: 1.64%                                             \n",
      "  Loss: 25%: 3.571904 50%: 4.009486 75%: 4.325815\n",
      "   Avg: 3.957460\n",
      "  Total Seq Acc: 3.233%\n",
      "    Avg Seq Acc: 42.656%\n",
      "Epoch: 8, Accuracy: 5.18%                                             \n",
      "  Loss: 25%: 3.171680 50%: 3.612419 75%: 3.961515\n",
      "   Avg: 3.558381\n",
      "  Total Seq Acc: 7.301%\n",
      "    Avg Seq Acc: 48.088%\n",
      "Epoch: 9, Accuracy: 10.70%                                             \n",
      "  Loss: 25%: 2.747878 50%: 3.077505 75%: 3.433291\n",
      "   Avg: 3.070574\n",
      "  Total Seq Acc: 12.099%\n",
      "    Avg Seq Acc: 55.879%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = BATCH_SIZE # pure memorisation so accumulation won't help\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.02})\n",
    "trainFor(50, target_seq_acc=0.50)\n",
    "# (34mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 6245\n",
      "  Total: 13054\n",
      "Epoch: 10, Accuracy: 13.81%                                             \n",
      "  Loss: 25%: 2.656691 50%: 2.988907 75%: 3.271646\n",
      "   Avg: 2.973416\n",
      "  Total Seq Acc: 19.721%\n",
      "    Avg Seq Acc: 68.222%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = BATCH_SIZE # pure memorisation so accumulation won't help\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.04})\n",
    "trainFor(50, target_seq_acc=0.60)\n",
    "# (5mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 9368\n",
      "  Total: 16177\n",
      "Epoch: 11, Accuracy: 17.17%                                             \n",
      "  Loss: 25%: 2.074187 50%: 2.373098 75%: 2.669538\n",
      "   Avg: 2.376195\n",
      "  Total Seq Acc: 27.387%\n",
      "    Avg Seq Acc: 76.537%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 28 # *14 close to 32\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.06})\n",
    "trainFor(50, target_seq_acc=0.65)\n",
    "# (5mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aiming for:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{unique}{tokens} &= \\frac{4174}{6808} = 61.31\\%  & \\text{sign pairs to text only used once} \\\\\n",
    "  \\frac{text}{tokens}   &= \\frac{5342}{6808}  = 78.47\\% & \\text{sign pairs to text unique text} \\\\\n",
    "  & & \\text{unique meaning the text is only used for one tokenID}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "i.e. there are six different signs which can be used for \"present\"  \n",
    "which means we're actually aiming for $96\\%$ effective accuracy $\\frac{75\\%}{78\\%}$\n",
    "\n",
    "But we don't want to over-fit either  \n",
    "Hence why we slowly introduce new concepts while still memorising vocabulary\n",
    "\n",
    "`target_seq_acc` includes the `EOS` token, which we of course we want to be right\n",
    "So our target should be $\\frac{61.31\\% + 100\\%}{2} = 80.65\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 12491\n",
      "  Total: 19300\n",
      "Epoch: 12, Accuracy: 20.01%                                             \n",
      "  Loss: 25%: 1.637285 50%: 1.861813 75%: 2.106627\n",
      "   Avg: 1.887390\n",
      "  Total Seq Acc: 32.244%\n",
      "    Avg Seq Acc: 80.065%\n",
      "Epoch: 13, Accuracy: 23.50%                                             \n",
      "  Loss: 25%: 1.358196 50%: 1.585695 75%: 1.840750\n",
      "   Avg: 1.606165\n",
      "  Total Seq Acc: 34.451%\n",
      "    Avg Seq Acc: 81.583%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 70 # *14 close to 64\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.08})\n",
    "trainFor(50, target_seq_acc=0.80655)\n",
    "# (11mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure memorisation is over, as the vocabulary has been sufficiently learnt  \n",
    "While it's not perfect, it will improve further over the later training, as the vocab remains in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 35132\n",
      "  Total: 41941\n",
      "Epoch: 14, Accuracy: 18.78%                                             \n",
      "  Loss: 25%: 1.329572 50%: 1.558345 75%: 1.814157\n",
      "   Avg: 1.585541\n",
      "  Total Seq Acc: 37.921%\n",
      "    Avg Seq Acc: 83.464%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.225})\n",
    "trainFor(50, target_seq_acc=0.6)\n",
    "# (10mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 70264\n",
      "  Total: 77073\n",
      "Epoch: 15, Accuracy: 20.15%                                             \n",
      "  Loss: 25%: 1.079807 50%: 1.281758 75%: 1.525836\n",
      "   Avg: 1.319893\n",
      "  Total Seq Acc: 41.399%\n",
      "    Avg Seq Acc: 85.418%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.45})\n",
    "trainFor(50, target_seq_acc=0.7)\n",
    "# (16mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 140528\n",
      "  Total: 147337\n",
      "Epoch: 16, Accuracy: 23.13%                                             \n",
      "  Loss: 25%: 0.838045 50%: 1.006275 75%: 1.225789\n",
      "   Avg: 1.055101\n",
      "  Total Seq Acc: 44.762%\n",
      "    Avg Seq Acc: 87.021%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.9})\n",
    "trainFor(50, target_seq_acc=0.75)\n",
    "# (30mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 140528\n",
      "  Total: 147337\n",
      "Epoch: 17, Accuracy: 26.92%                                             \n",
      "  Loss: 25%: 0.662440 50%: 0.804522 75%: 0.992638\n",
      "   Avg: 0.856268\n",
      "  Total Seq Acc: 45.779%\n",
      "    Avg Seq Acc: 87.521%\n",
      "Epoch: 18, Accuracy: 29.49%                                             \n",
      "  Loss: 25%: 0.562557 50%: 0.685241 75%: 0.851517\n",
      "   Avg: 0.737888\n",
      "  Total Seq Acc: 47.928%\n",
      "    Avg Seq Acc: 88.370%\n",
      "Epoch: 19, Accuracy: 31.52%                                             \n",
      "  Loss: 25%: 0.491418 50%: 0.597656 75%: 0.744295\n",
      "   Avg: 0.650201\n",
      "  Total Seq Acc: 48.513%\n",
      "    Avg Seq Acc: 88.522%\n",
      "Epoch: 20, Accuracy: 32.73%                                             \n",
      "  Loss: 25%: 0.447802 50%: 0.545643 75%: 0.681414\n",
      "   Avg: 0.597653\n",
      "  Total Seq Acc: 49.436%\n",
      "    Avg Seq Acc: 88.904%\n",
      "Epoch: 21, Accuracy: 34.06%                                             \n",
      "  Loss: 25%: 0.411880 50%: 0.501514 75%: 0.623499\n",
      "   Avg: 0.550958\n",
      "  Total Seq Acc: 49.931%\n",
      "    Avg Seq Acc: 89.166%\n",
      "Epoch: 22, Accuracy: 35.01%                                             \n",
      "  Loss: 25%: 0.384031 50%: 0.466311 75%: 0.577149\n",
      "   Avg: 0.513654\n",
      "  Total Seq Acc: 50.270%\n",
      "    Avg Seq Acc: 89.304%\n",
      "Epoch: 23, Accuracy: 35.84%                                             \n",
      "  Loss: 25%: 0.362658 50%: 0.438559 75%: 0.541728\n",
      "   Avg: 0.483325\n",
      "  Total Seq Acc: 50.894%\n",
      "    Avg Seq Acc: 89.507%\n",
      "Epoch: 24, Accuracy: 36.37%                                             \n",
      "  Loss: 25%: 0.343976 50%: 0.415291 75%: 0.513873\n",
      "   Avg: 0.460318\n",
      "  Total Seq Acc: 50.912%\n",
      "    Avg Seq Acc: 89.472%\n",
      "Epoch: 25, Accuracy: 37.01%                                             \n",
      "  Loss: 25%: 0.332605 50%: 0.403996 75%: 0.502858\n",
      "   Avg: 0.449104\n",
      "  Total Seq Acc: 51.041%\n",
      "    Avg Seq Acc: 89.575%\n",
      "Epoch: 26, Accuracy: 37.62%                                             \n",
      "  Loss: 25%: 0.317929 50%: 0.386799 75%: 0.481079\n",
      "   Avg: 0.429650\n",
      "  Total Seq Acc: 51.197%\n",
      "    Avg Seq Acc: 89.643%\n",
      "Epoch: 27, Accuracy: 37.94%                                             \n",
      "  Loss: 25%: 0.308349 50%: 0.372526 75%: 0.456689\n",
      "   Avg: 0.413500\n",
      "  Total Seq Acc: 51.577%\n",
      "    Avg Seq Acc: 89.717%\n",
      "Epoch: 28, Accuracy: 38.33%                                             \n",
      "  Loss: 25%: 0.296087 50%: 0.356619 75%: 0.437291\n",
      "   Avg: 0.396411\n",
      "  Total Seq Acc: 51.643%\n",
      "    Avg Seq Acc: 89.830%\n",
      "Epoch: 29, Accuracy: 38.65%                                             \n",
      "  Loss: 25%: 0.289713 50%: 0.347075 75%: 0.422274\n",
      "   Avg: 0.384405\n",
      "  Total Seq Acc: 51.648%\n",
      "    Avg Seq Acc: 89.834%\n",
      "Epoch: 30, Accuracy: 39.05%                                             \n",
      "  Loss: 25%: 0.279989 50%: 0.338640 75%: 0.412372\n",
      "   Avg: 0.374840\n",
      "  Total Seq Acc: 52.192%\n",
      "    Avg Seq Acc: 89.935%\n",
      "Epoch: 31, Accuracy: 39.41%                                             \n",
      "  Loss: 25%: 0.273525 50%: 0.329196 75%: 0.399910\n",
      "   Avg: 0.365167\n",
      "  Total Seq Acc: 52.089%\n",
      "    Avg Seq Acc: 89.978%\n",
      "Epoch: 32, Accuracy: 39.70%                                             \n",
      "  Loss: 25%: 0.267469 50%: 0.320570 75%: 0.389043\n",
      "   Avg: 0.355207\n",
      "  Total Seq Acc: 51.960%\n",
      "    Avg Seq Acc: 89.957%\n",
      "Epoch: 33, Accuracy: 39.86%                                             \n",
      "  Loss: 25%: 0.264502 50%: 0.317547 75%: 0.383463\n",
      "   Avg: 0.350738\n",
      "  Total Seq Acc: 51.880%\n",
      "    Avg Seq Acc: 89.958%\n",
      "Epoch: 34, Accuracy: 40.03%                                             \n",
      "  Loss: 25%: 0.258477 50%: 0.310631 75%: 0.377177\n",
      "   Avg: 0.342286\n",
      "  Total Seq Acc: 52.165%\n",
      "    Avg Seq Acc: 90.004%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 1022 # *14 close to 1024\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.9})\n",
    "trainFor(50, target_p50_loss=0.2, target_seq_acc=0.90)\n",
    "# (533mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - noise: 15614\n",
      "  Total: 22423\n",
      "  Total Seq Acc: 52.165%\n",
      "    Avg Seq Acc: 90.004%\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "  Total: 6809\n",
      "  Total Seq Acc: 77.486%\n",
      "    Avg Seq Acc: 92.470%\n",
      "Concepts loaded;\n",
      "    - noise: 15614\n",
      "  Total: 15615\n",
      "  Total Seq Acc: 41.121%\n",
      "    Avg Seq Acc: 88.925%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.88925186383812, 0.4112071725904579)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_concept({\"vocabulary\": 1.0, \"noise\": 0.1}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"vocabulary\": 1.0}, validation=True)\n",
    "validate()\n",
    "apply_concept({\"noise\": 0.1}, validation=True)\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has successfully applied a vocabulary swap we must start the grammatical transformation training.  \n",
    "First we will start by just teaching it which words to ignore as they do not exist in Auslan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 10000\n",
      "  Total: 16809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 45000\n",
      "  Total: 51809\n",
      "Epoch: 35, Accuracy: 9.80%                                             \n",
      "  Loss: 25%: 5.300752 50%: 5.609476 75%: 5.946348\n",
      "   Avg: 5.744106\n",
      "  Total Seq Acc: 31.804%\n",
      "    Avg Seq Acc: 58.800%\n",
      "Epoch: 36, Accuracy: 9.81%                                             \n",
      "  Loss: 25%: 4.243643 50%: 4.516359 75%: 4.793437\n",
      "   Avg: 4.508092\n",
      "  Total Seq Acc: 31.638%\n",
      "    Avg Seq Acc: 64.020%\n",
      "Epoch: 37, Accuracy: 9.89%                                             \n",
      "  Loss: 25%: 3.599592 50%: 3.846125 75%: 4.072579\n",
      "   Avg: 3.822467\n",
      "  Total Seq Acc: 32.019%\n",
      "    Avg Seq Acc: 64.472%\n",
      "Epoch: 38, Accuracy: 10.03%                                             \n",
      "  Loss: 25%: 3.236534 50%: 3.473460 75%: 3.700928\n",
      "   Avg: 3.461110\n",
      "  Total Seq Acc: 31.751%\n",
      "    Avg Seq Acc: 65.270%\n",
      "Epoch: 39, Accuracy: 10.03%                                             \n",
      "  Loss: 25%: 3.030043 50%: 3.260919 75%: 3.484393\n",
      "   Avg: 3.248439\n",
      "  Total Seq Acc: 32.120%\n",
      "    Avg Seq Acc: 71.423%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 4088 # *14 close to 4096\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.45})\n",
    "trainFor(20, target_p50_loss=0.2, target_seq_acc=0.70)\n",
    "# (57mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 10000\n",
      "  Total: 16809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - word-masking: 90000\n",
      "  Total: 96809\n",
      "Epoch: 40, Accuracy: 5.95%                                             \n",
      "  Loss: 25%: 2.783838 50%: 3.019561 75%: 3.248209\n",
      "   Avg: 3.012249\n",
      "  Total Seq Acc: 31.828%\n",
      "    Avg Seq Acc: 69.566%\n",
      "Epoch: 41, Accuracy: 6.10%                                             \n",
      "  Loss: 25%: 2.493118 50%: 2.705811 75%: 2.920518\n",
      "   Avg: 2.705100\n",
      "  Total Seq Acc: 32.019%\n",
      "    Avg Seq Acc: 72.888%\n",
      "Epoch: 42, Accuracy: 6.09%                                             \n",
      "  Loss: 25%: 2.316473 50%: 2.525594 75%: 2.723895\n",
      "   Avg: 2.516568\n",
      "  Total Seq Acc: 32.310%\n",
      "    Avg Seq Acc: 76.054%\n",
      "Epoch: 43, Accuracy: 6.19%                                             \n",
      "  Loss: 25%: 2.155381 50%: 2.360453 75%: 2.556274\n",
      "   Avg: 2.357171\n",
      "  Total Seq Acc: 32.013%\n",
      "    Avg Seq Acc: 75.060%\n",
      "Epoch: 44, Accuracy: 6.24%                                             \n",
      "  Loss: 25%: 2.032776 50%: 2.226068 75%: 2.420145\n",
      "   Avg: 2.224036\n",
      "  Total Seq Acc: 32.132%\n",
      "    Avg Seq Acc: 77.114%\n",
      "> KeyboardInterrupt"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 4088 # *14 close to 4096\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"word-masking\": 0.9})\n",
    "trainFor(20, target_p50_loss=0.2, target_seq_acc=0.9)\n",
    "# (mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancelled early because we don't want to put too much focus into just the word masking  \n",
    "The model already has a strong grasp on the concept and can start learning word arranging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 250000\n",
      "  Total: 256809\n",
      "Epoch: 45, Accuracy: 1.74%                                             \n",
      "  Loss: 25%: 5.043064 50%: 5.283244 75%: 5.601397\n",
      "   Avg: 5.440462\n",
      "  Total Seq Acc: 8.914%\n",
      "    Avg Seq Acc: 30.513%\n",
      "Epoch: 46, Accuracy: 1.53%                                             \n",
      "  Loss: 25%: 4.573521 50%: 4.699400 75%: 4.836210\n",
      "   Avg: 4.709103\n",
      "  Total Seq Acc: 8.509%\n",
      "    Avg Seq Acc: 31.355%\n",
      "Epoch: 47, Accuracy: 1.31%                                             \n",
      "  Loss: 25%: 4.356220 50%: 4.481114 75%: 4.614680\n",
      "   Avg: 4.489703\n",
      "  Total Seq Acc: 7.810%\n",
      "    Avg Seq Acc: 32.108%\n",
      "Epoch: 48, Accuracy: 1.12%                                             \n",
      "  Loss: 25%: 4.191215 50%: 4.323497 75%: 4.463085\n",
      "   Avg: 4.331535\n",
      "  Total Seq Acc: 7.326%\n",
      "    Avg Seq Acc: 32.913%\n",
      "Epoch: 49, Accuracy: 1.14%                                             \n",
      "  Loss: 25%: 4.053386 50%: 4.192237 75%: 4.341026\n",
      "   Avg: 4.202481\n",
      "  Total Seq Acc: 7.256%\n",
      "    Avg Seq Acc: 33.989%\n",
      "Epoch: 50, Accuracy: 1.20%                                             \n",
      "  Loss: 25%: 3.917146 50%: 4.059803 75%: 4.214868\n",
      "   Avg: 4.071232\n",
      "  Total Seq Acc: 7.045%\n",
      "    Avg Seq Acc: 35.113%\n",
      "Epoch: 51, Accuracy: 1.26%                                             \n",
      "  Loss: 25%: 3.773987 50%: 3.923282 75%: 4.081618\n",
      "   Avg: 3.933274\n",
      "  Total Seq Acc: 7.143%\n",
      "    Avg Seq Acc: 36.399%\n",
      "Epoch: 52, Accuracy: 1.31%                                             \n",
      "  Loss: 25%: 3.656388 50%: 3.806739 75%: 3.972433\n",
      "   Avg: 3.821005\n",
      "  Total Seq Acc: 7.175%\n",
      "    Avg Seq Acc: 37.731%\n",
      "Epoch: 53, Accuracy: 1.32%                                             \n",
      "  Loss: 25%: 3.552227 50%: 3.709776 75%: 3.880910\n",
      "   Avg: 3.723255\n",
      "  Total Seq Acc: 7.244%\n",
      "    Avg Seq Acc: 38.917%\n",
      "Epoch: 54, Accuracy: 1.30%                                             \n",
      "  Loss: 25%: 3.459133 50%: 3.619415 75%: 3.791599\n",
      "   Avg: 3.631819\n",
      "  Total Seq Acc: 7.397%\n",
      "    Avg Seq Acc: 39.801%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.5})\n",
    "trainFor(10, target_p50_loss=0.2, target_seq_acc=0.5)\n",
    "# (551mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 300000\n",
      "  Total: 306809\n",
      "Epoch: 55, Accuracy: 1.08%                                             \n",
      "  Loss: 25%: 3.428736 50%: 3.605553 75%: 3.783802\n",
      "   Avg: 3.614931\n",
      "  Total Seq Acc: 7.166%\n",
      "    Avg Seq Acc: 40.403%\n",
      "Epoch: 56, Accuracy: 1.06%                                             \n",
      "  Loss: 25%: 3.335988 50%: 3.500705 75%: 3.674894\n",
      "   Avg: 3.511020\n",
      "  Total Seq Acc: 7.180%\n",
      "    Avg Seq Acc: 40.782%\n",
      "Epoch: 57, Accuracy: 1.06%                                             \n",
      "  Loss: 25%: 3.262367 50%: 3.426763 75%: 3.606675\n",
      "   Avg: 3.441214\n",
      "  Total Seq Acc: 7.090%\n",
      "    Avg Seq Acc: 41.457%\n",
      "Epoch: 58, Accuracy: 1.04%                                             \n",
      "  Loss: 25%: 3.187698 50%: 3.354445 75%: 3.538030\n",
      "   Avg: 3.369233\n",
      "  Total Seq Acc: 6.971%\n",
      "    Avg Seq Acc: 42.178%\n",
      "Epoch: 59, Accuracy: 1.01%                                             \n",
      "  Loss: 25%: 3.112078 50%: 3.279598 75%: 3.462682\n",
      "   Avg: 3.294389\n",
      "  Total Seq Acc: 6.994%\n",
      "    Avg Seq Acc: 43.202%\n",
      "Epoch: 60, Accuracy: 1.03%                                             \n",
      "  Loss: 25%: 3.034516 50%: 3.203061 75%: 3.386942\n",
      "   Avg: 3.215841\n",
      "  Total Seq Acc: 6.925%\n",
      "    Avg Seq Acc: 44.261%\n",
      "Epoch: 61, Accuracy: 1.02%                                             \n",
      "  Loss: 25%: 2.943640 50%: 3.120018 75%: 3.314670\n",
      "   Avg: 3.136207\n",
      "  Total Seq Acc: 6.844%\n",
      "    Avg Seq Acc: 45.265%\n",
      "Epoch: 62, Accuracy: 1.03%                                             \n",
      "  Loss: 25%: 2.864992 50%: 3.043318 75%: 3.233272\n",
      "   Avg: 3.057432\n",
      "  Total Seq Acc: 6.738%\n",
      "    Avg Seq Acc: 46.313%\n",
      "Epoch: 63, Accuracy: 1.04%                                             \n",
      "  Loss: 25%: 2.785462 50%: 2.968136 75%: 3.159860\n",
      "   Avg: 2.979809\n",
      "  Total Seq Acc: 6.708%\n",
      "    Avg Seq Acc: 47.302%\n",
      "Epoch: 64, Accuracy: 1.06%                                             \n",
      "  Loss: 25%: 2.712641 50%: 2.892994 75%: 3.087849\n",
      "   Avg: 2.906304\n",
      "  Total Seq Acc: 6.835%\n",
      "    Avg Seq Acc: 48.330%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.6})\n",
    "trainFor(10, target_p50_loss=0.2, target_seq_acc=0.6)\n",
    "# (649mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 65, Accuracy: 0.70%                                             \n",
      "  Loss: 25%: 2.682333 50%: 2.871508 75%: 3.074587\n",
      "   Avg: 2.884193\n",
      "  Total Seq Acc: 6.744%\n",
      "    Avg Seq Acc: 48.775%\n",
      "Epoch: 66, Accuracy: 0.73%                                             \n",
      "  Loss: 25%: 2.602419 50%: 2.783997 75%: 2.978259\n",
      "   Avg: 2.797313\n",
      "  Total Seq Acc: 6.784%\n",
      "    Avg Seq Acc: 49.538%\n",
      "Epoch: 67, Accuracy: 0.73%                                             \n",
      "  Loss: 25%: 2.537791 50%: 2.718444 75%: 2.912007\n",
      "   Avg: 2.731926\n",
      "  Total Seq Acc: 6.883%\n",
      "    Avg Seq Acc: 50.525%\n",
      "Epoch: 68, Accuracy: 0.74%                                             \n",
      "  Loss: 25%: 2.467539 50%: 2.650765 75%: 2.846792\n",
      "   Avg: 2.663782\n",
      "  Total Seq Acc: 7.027%\n",
      "    Avg Seq Acc: 51.586%\n",
      "Epoch: 69, Accuracy: 0.76%                                             \n",
      "  Loss: 25%: 2.397745 50%: 2.582291 75%: 2.775341\n",
      "   Avg: 2.593369\n",
      "  Total Seq Acc: 7.110%\n",
      "    Avg Seq Acc: 52.768%\n",
      "Epoch: 70, Accuracy: 0.82%                                             \n",
      "  Loss: 25%: 2.324029 50%: 2.505407 75%: 2.699867\n",
      "   Avg: 2.518766\n",
      "  Total Seq Acc: 7.263%\n",
      "    Avg Seq Acc: 53.996%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(6, target_p50_loss=0.2, target_seq_acc=0.75)\n",
    "# (568mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 71, Accuracy: 0.81%                                             \n",
      "  Loss: 25%: 2.314888 50%: 2.502601 75%: 2.708355\n",
      "   Avg: 2.521973\n",
      "  Total Seq Acc: 7.245%\n",
      "    Avg Seq Acc: 54.242%\n",
      "Epoch: 72, Accuracy: 0.84%                                             \n",
      "  Loss: 25%: 2.241149 50%: 2.418887 75%: 2.607680\n",
      "   Avg: 2.430881\n",
      "  Total Seq Acc: 7.272%\n",
      "    Avg Seq Acc: 54.987%\n",
      "Epoch: 73, Accuracy: 0.87%                                             \n",
      "  Loss: 25%: 2.194081 50%: 2.370080 75%: 2.560852\n",
      "   Avg: 2.383077\n",
      "  Total Seq Acc: 7.323%\n",
      "    Avg Seq Acc: 55.757%\n",
      "Epoch: 74, Accuracy: 0.89%                                             \n",
      "  Loss: 25%: 2.140822 50%: 2.313307 75%: 2.498226\n",
      "   Avg: 2.325857\n",
      "  Total Seq Acc: 7.409%\n",
      "    Avg Seq Acc: 56.885%\n",
      "Epoch: 75, Accuracy: 0.92%                                             \n",
      "  Loss: 25%: 2.063219 50%: 2.231266 75%: 2.406653\n",
      "   Avg: 2.242457\n",
      "  Total Seq Acc: 7.476%\n",
      "    Avg Seq Acc: 58.416%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=0.2, target_seq_acc=0.8)\n",
    "# (475mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 76, Accuracy: 0.92%                                             \n",
      "  Loss: 25%: 2.046665 50%: 2.215688 75%: 2.399971\n",
      "   Avg: 2.231916\n",
      "  Total Seq Acc: 7.506%\n",
      "    Avg Seq Acc: 58.765%\n",
      "Epoch: 77, Accuracy: 0.96%                                             \n",
      "  Loss: 25%: 1.970358 50%: 2.127085 75%: 2.290505\n",
      "   Avg: 2.136366\n",
      "  Total Seq Acc: 7.543%\n",
      "    Avg Seq Acc: 59.686%\n",
      "Epoch: 78, Accuracy: 0.97%                                             \n",
      "  Loss: 25%: 1.921430 50%: 2.077863 75%: 2.235225\n",
      "   Avg: 2.084244\n",
      "  Total Seq Acc: 7.601%\n",
      "    Avg Seq Acc: 60.986%\n",
      "Epoch: 79, Accuracy: 0.98%                                             \n",
      "  Loss: 25%: 1.871995 50%: 2.023464 75%: 2.180216\n",
      "   Avg: 2.030972\n",
      "  Total Seq Acc: 7.596%\n",
      "    Avg Seq Acc: 61.886%\n",
      "Epoch: 80, Accuracy: 1.01%                                             \n",
      "  Loss: 25%: 1.819543 50%: 1.970200 75%: 2.127292\n",
      "   Avg: 1.978320\n",
      "  Total Seq Acc: 7.719%\n",
      "    Avg Seq Acc: 63.217%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=0.2, target_seq_acc=0.8)\n",
    "# (474mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 81, Accuracy: 1.04%                                             \n",
      "  Loss: 25%: 1.829469 50%: 1.987885 75%: 2.159871\n",
      "   Avg: 2.007925\n",
      "  Total Seq Acc: 7.758%\n",
      "    Avg Seq Acc: 63.655%\n",
      "Epoch: 82, Accuracy: 1.06%                                             \n",
      "  Loss: 25%: 1.761185 50%: 1.906973 75%: 2.061434\n",
      "   Avg: 1.916527\n",
      "  Total Seq Acc: 7.766%\n",
      "    Avg Seq Acc: 64.108%\n",
      "Epoch: 83, Accuracy: 1.08%                                             \n",
      "  Loss: 25%: 1.730055 50%: 1.877588 75%: 2.032675\n",
      "   Avg: 1.886860\n",
      "  Total Seq Acc: 7.779%\n",
      "    Avg Seq Acc: 64.661%\n",
      "Epoch: 84, Accuracy: 1.11%                                             \n",
      "  Loss: 25%: 1.695751 50%: 1.840608 75%: 1.996137\n",
      "   Avg: 1.850816\n",
      "  Total Seq Acc: 7.888%\n",
      "    Avg Seq Acc: 65.719%\n",
      "Epoch: 85, Accuracy: 1.14%                                             \n",
      "  Loss: 25%: 1.657749 50%: 1.804256 75%: 1.958041\n",
      "   Avg: 1.813987\n",
      "  Total Seq Acc: 7.981%\n",
      "    Avg Seq Acc: 66.537%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=0.2, target_seq_acc=0.8)\n",
    "# (474mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 86, Accuracy: 1.15%                                             \n",
      "  Loss: 25%: 1.675644 50%: 1.827500 75%: 1.994921\n",
      "   Avg: 1.849289\n",
      "  Total Seq Acc: 8.000%\n",
      "    Avg Seq Acc: 66.860%\n",
      "Epoch: 87, Accuracy: 1.17%                                             \n",
      "  Loss: 25%: 1.615581 50%: 1.757754 75%: 1.911210\n",
      "   Avg: 1.769092\n",
      "  Total Seq Acc: 7.993%\n",
      "    Avg Seq Acc: 67.050%\n",
      "Epoch: 88, Accuracy: 1.22%                                             \n",
      "  Loss: 25%: 1.590176 50%: 1.732046 75%: 1.881348\n",
      "   Avg: 1.743233\n",
      "  Total Seq Acc: 8.044%\n",
      "    Avg Seq Acc: 67.473%\n",
      "Epoch: 89, Accuracy: 1.25%                                             \n",
      "  Loss: 25%: 1.562519 50%: 1.704608 75%: 1.854645\n",
      "   Avg: 1.715583\n",
      "  Total Seq Acc: 8.192%\n",
      "    Avg Seq Acc: 68.291%\n",
      "Epoch: 90, Accuracy: 1.30%                                             \n",
      "  Loss: 25%: 1.533324 50%: 1.672347 75%: 1.825556\n",
      "   Avg: 1.685914\n",
      "  Total Seq Acc: 8.203%\n",
      "    Avg Seq Acc: 68.833%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=0.2, target_seq_acc=0.8)\n",
    "# (473mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 91, Accuracy: 1.27%                                             \n",
      "  Loss: 25%: 1.567690 50%: 1.723511 75%: 1.903988\n",
      "   Avg: 1.762427\n",
      "  Total Seq Acc: 8.333%\n",
      "    Avg Seq Acc: 69.191%\n",
      "Epoch: 92, Accuracy: 1.34%                                             \n",
      "  Loss: 25%: 1.499688 50%: 1.637657 75%: 1.787199\n",
      "   Avg: 1.651440\n",
      "  Total Seq Acc: 8.353%\n",
      "    Avg Seq Acc: 69.426%\n",
      "Epoch: 93, Accuracy: 1.36%                                             \n",
      "  Loss: 25%: 1.480775 50%: 1.618415 75%: 1.769462\n",
      "   Avg: 1.632100\n",
      "  Total Seq Acc: 8.456%\n",
      "    Avg Seq Acc: 70.202%\n",
      "Epoch: 94, Accuracy: 1.39%                                             \n",
      "  Loss: 25%: 1.456432 50%: 1.596672 75%: 1.747928\n",
      "   Avg: 1.610726\n",
      "  Total Seq Acc: 8.504%\n",
      "    Avg Seq Acc: 70.455%\n",
      "Epoch: 95, Accuracy: 1.46%                                             \n",
      "  Loss: 25%: 1.434069 50%: 1.570207 75%: 1.721674\n",
      "   Avg: 1.586828\n",
      "  Total Seq Acc: 8.731%\n",
      "    Avg Seq Acc: 71.352%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=0.2, target_seq_acc=0.8)\n",
    "# (474mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 96, Accuracy: 1.41%                                             \n",
      "  Loss: 25%: 1.470657 50%: 1.624199 75%: 1.803982\n",
      "   Avg: 1.663073\n",
      "  Total Seq Acc: 8.668%\n",
      "    Avg Seq Acc: 71.137%\n",
      "Epoch: 97, Accuracy: 1.50%                                             \n",
      "  Loss: 25%: 1.406769 50%: 1.543143 75%: 1.693135\n",
      "   Avg: 1.558744\n",
      "  Total Seq Acc: 8.726%\n",
      "    Avg Seq Acc: 71.208%\n",
      "Epoch: 98, Accuracy: 1.54%                                             \n",
      "  Loss: 25%: 1.387624 50%: 1.525135 75%: 1.675027\n",
      "   Avg: 1.540241\n",
      "  Total Seq Acc: 8.842%\n",
      "    Avg Seq Acc: 72.147%\n",
      "Epoch: 99, Accuracy: 1.56%                                             \n",
      "  Loss: 25%: 1.368311 50%: 1.505243 75%: 1.655955\n",
      "   Avg: 1.521673\n",
      "  Total Seq Acc: 8.900%\n",
      "    Avg Seq Acc: 72.388%\n",
      "Epoch: 100, Accuracy: 1.63%                                             \n",
      "  Loss: 25%: 1.350048 50%: 1.487024 75%: 1.631964\n",
      "   Avg: 1.501735\n",
      "  Total Seq Acc: 9.074%\n",
      "    Avg Seq Acc: 73.266%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=1.0, target_seq_acc=0.8)\n",
    "# (474mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 101, Accuracy: 1.55%                                             \n",
      "  Loss: 25%: 1.400047 50%: 1.558946 75%: 1.752922\n",
      "   Avg: 1.617956\n",
      "  Total Seq Acc: 8.983%\n",
      "    Avg Seq Acc: 72.945%\n",
      "Epoch: 102, Accuracy: 1.65%                                             \n",
      "  Loss: 25%: 1.326405 50%: 1.460945 75%: 1.609611\n",
      "   Avg: 1.478250\n",
      "  Total Seq Acc: 9.132%\n",
      "    Avg Seq Acc: 73.535%\n",
      "Epoch: 103, Accuracy: 1.68%                                             \n",
      "  Loss: 25%: 1.311099 50%: 1.445052 75%: 1.595631\n",
      "   Avg: 1.463833\n",
      "  Total Seq Acc: 9.131%\n",
      "    Avg Seq Acc: 73.605%\n",
      "Epoch: 104, Accuracy: 1.70%                                             \n",
      "  Loss: 25%: 1.295619 50%: 1.429284 75%: 1.577475\n",
      "   Avg: 1.447973\n",
      "  Total Seq Acc: 9.241%\n",
      "    Avg Seq Acc: 74.299%\n",
      "Epoch: 105, Accuracy: 1.75%                                             \n",
      "  Loss: 25%: 1.278585 50%: 1.411813 75%: 1.561065\n",
      "   Avg: 1.430726\n",
      "  Total Seq Acc: 9.331%\n",
      "    Avg Seq Acc: 74.608%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=1.0, target_seq_acc=0.8)\n",
    "# (475mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 50000\n",
      "  Total: 56809\n",
      "Concepts loaded;\n",
      "    - vocabulary: 6808\n",
      "    - grammar: 450000\n",
      "  Total: 456809\n",
      "Epoch: 106, Accuracy: 1.70%                                             \n",
      "  Loss: 25%: 1.326098 50%: 1.478259 75%: 1.661936\n",
      "   Avg: 1.524481\n",
      "  Total Seq Acc: 9.233%\n",
      "    Avg Seq Acc: 73.959%\n",
      "Epoch: 107, Accuracy: 1.80%                                             \n",
      "  Loss: 25%: 1.259377 50%: 1.391462 75%: 1.540108\n",
      "   Avg: 1.411018\n",
      "  Total Seq Acc: 9.300%\n",
      "    Avg Seq Acc: 74.631%\n",
      "Epoch: 108, Accuracy: 1.79%                                             \n",
      "  Loss: 25%: 1.244987 50%: 1.377174 75%: 1.527523\n",
      "   Avg: 1.397827\n",
      "  Total Seq Acc: 9.400%\n",
      "    Avg Seq Acc: 75.063%\n",
      "Epoch: 109, Accuracy: 1.88%                                             \n",
      "  Loss: 25%: 1.234408 50%: 1.364696 75%: 1.512952\n",
      "   Avg: 1.385449\n",
      "  Total Seq Acc: 9.425%\n",
      "    Avg Seq Acc: 75.147%\n",
      "Epoch: 110, Accuracy: 1.91%                                             \n",
      "  Loss: 25%: 1.217921 50%: 1.349436 75%: 1.497795\n",
      "   Avg: 1.369779\n",
      "  Total Seq Acc: 9.495%\n",
      "    Avg Seq Acc: 75.518%\n"
     ]
    }
   ],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9})\n",
    "trainFor(5, target_p50_loss=1.0, target_seq_acc=0.8)\n",
    "# (475mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCUMULATION_STEPS = 8190 # *14 close to 8192\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.1, \"word-pairs\": 0.1}, validation=True)\n",
    "apply_concept({\"vocabulary\": 1.0, \"grammar\": 0.9, \"word-pairs\": 0.9})\n",
    "trainFor(5, target_p50_loss=1.0, target_seq_acc=0.8)\n",
    "# (mins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
